{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff290305",
   "metadata": {},
   "source": [
    "# A4: Do You Agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf88c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2315139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0527f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e53f0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb94fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308592a1",
   "metadata": {},
   "source": [
    "### Dataset Details — WikiText-2\n",
    "\n",
    "For this assignment, I used the **WikiText-2** dataset, a language modeling corpus extracted from English Wikipedia articles that are labeled as “Good” or “Featured.” This dataset is designed to support long-range dependency tasks and contains more natural text than older benchmarks like Penn Treebank. \n",
    "\n",
    "- **Official Name:** WikiText-2  \n",
    "- **Dataset Source:** Hugging Face — Salesforce/wikitext  \n",
    "  https://huggingface.co/datasets/Salesforce/wikitext \n",
    "- **Original Paper:** Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). *Pointer Sentinel Mixture Models*. ICLR. arXiv:1609.07843 (introduces the WikiText dataset). \n",
    "\n",
    "**Academic citation (APA):**\n",
    "> Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). *Pointer sentinel mixture models*. ICLR. arXiv:1609.07843. Retrieved from https://arxiv.org/abs/1609.07843\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320ad280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c359fbf3abac441c849e159092bd80e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-v1/test-00000-of-00001.parque(…):   0%|          | 0.00/685k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT\\NLP_Assignment\\NLP\\A4\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tisab\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29259c3b19a643439c4fdec8d2aca1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-v1/train-00000-of-00001.parqu(…):   0%|          | 0.00/6.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d30f73e9d140799bd173b8bb2faa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "wikitext-2-v1/validation-00000-of-00001.(…):   0%|          | 0.00/618k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ac1e1cc1d647c7adb29a4139e15927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbebfe3ebd346959175c29db3b2f35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8100fa02d841d0bf2cdb28b839f79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Original size: 36718 samples\n",
      "\n",
      "Sample text from dataset:\n",
      "--------------------------------------------------\n",
      " The game 's battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wikitext', 'wikitext-2-v1', split='train')\n",
    "\n",
    "print(f\"Dataset loaded. Original size: {len(dataset)} samples\")\n",
    "\n",
    "\n",
    "print(\"\\nSample text from dataset:\")\n",
    "print(\"-\" * 50)\n",
    "print(dataset[10]['text']) # Print a random sample to check quality\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0433fa",
   "metadata": {},
   "source": [
    "# Task 1: Training BERT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37579d1a",
   "metadata": {},
   "source": [
    "#### Preprocessing: Masked Language Modeling (MLM) Logic\n",
    "\n",
    "To train BERT, the model needs to learn how to predict missing words based on their context. I built a custom `Dataset` class that manually implements the BERT masking strategy. \n",
    "\n",
    "Instead of just masking words simply, I followed the original BERT paper's logic:\n",
    "1. I select 15% of the tokens in a sentence to be masked.\n",
    "2. Of that 15%, 80% are replaced with a `[MASK]` token, 10% are replaced with a random word, and 10% are left as the original word.\n",
    "\n",
    "I also set the labels for all the *unmasked* words to `-100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fcfb9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing DataLoader...\n",
      "DataLoader ready. Batch size: 32\n",
      "Input shape: torch.Size([32, 128])\n",
      "Labels shape: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuration\n",
    "# We define our hyperparameters here.\n",
    "MAX_LEN = 128        # Maximum length of a sentence (tokens)\n",
    "BATCH_SIZE = 32      # Process 32 samples at a time\n",
    "MAX_MASK = 20        # Maximum number of tokens to mask per sequence\n",
    "VOCAB_SIZE = 30522   # Standard BERT vocab size\n",
    "\n",
    "# 2. Initialize Tokenizer\n",
    "# We use the standard BERT tokenizer to convert text -> numbers\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 3. Custom Dataset Class\n",
    "# This class handles the logic of reading text and creating masks manually.\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Get the text\n",
    "        text = self.data[index]['text']\n",
    "\n",
    "        # 2. Tokenize\n",
    "        # - truncation=True: Cut off if longer than max_len\n",
    "        # - padding='max_length': Add [PAD] if shorter than max_len\n",
    "        # - return_tensors='pt': Return PyTorch tensors\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Remove the batch dimension added by tokenizer (1, 128) -> (128)\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        # 3. Create Masking (MLM) - \"From Scratch\" Logic\n",
    "        # We create a copy of input_ids to be our \"labels\" (the answers)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Create a random probability matrix for masking\n",
    "        # We mask approx 15% of tokens, but NOT [CLS] (101), [SEP] (102), or [PAD] (0)\n",
    "        probability_matrix = torch.full(labels.shape, 0.15)\n",
    "        \n",
    "        # Create special token mask\n",
    "        special_tokens_mask = (input_ids == tokenizer.pad_token_id) | \\\n",
    "                              (input_ids == tokenizer.cls_token_id) | \\\n",
    "                              (input_ids == tokenizer.sep_token_id)\n",
    "        \n",
    "        # Set probability of masking special tokens to 0\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "\n",
    "        # Select indices to mask based on probabilities\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "\n",
    "        # Set labels for NON-masked tokens to -100 (so the loss function ignores them)\n",
    "        labels[~masked_indices] = -100 \n",
    "\n",
    "        # 4. Apply the 80-10-10 Rule\n",
    "        # 80% of the time: Replace with [MASK] token\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        input_ids[indices_replaced] = tokenizer.mask_token_id\n",
    "\n",
    "        # 10% of the time: Replace with a random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "        input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The remaining 10% are kept as original words (but still predicted)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# 4. Create DataLoader\n",
    "print(\"Preparing DataLoader...\")\n",
    "train_dataset = BERTDataset(dataset, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 5. Verify a single batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"DataLoader ready. Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Input shape: {sample_batch['input_ids'].shape}\") \n",
    "print(f\"Labels shape: {sample_batch['labels'].shape}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdc303",
   "metadata": {},
   "source": [
    "## Building the BERT Architecture\n",
    "I built the Transformer components using PyTorch `nn.Module`:\n",
    "* **BERTEmbedding**: Combines token embeddings, position embeddings, and segment embeddings.\n",
    "* **MultiHeadAttention**: Calculates the scaled dot-product attention to help the model focus on different parts of the sentence.\n",
    "* **EncoderLayer**: Stacks the attention mechanism with a feed-forward network, layer normalization, and dropout.\n",
    "* **BERT**: The final wrapper that connects the embedding layer, multiple encoder layers, and adds a linear classifier on top to predict our masked vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aacd48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  BERT Model Architecture\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, n_segments=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        # Create position tensor: [0, 1, 2, ..., seq_len-1]\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)\n",
    "        \n",
    "        # Sum all embeddings\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.drop(self.norm(embedding))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # Linear projections and split into heads\n",
    "        # Shape: [batch_size, seq_len, n_heads, d_k]\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        # Scores: [batch_size, n_heads, seq_len, seq_len]\n",
    "        scores = torch.matmul(q_s, k_s.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply Mask (Prevent looking at padding)\n",
    "        # attn_mask shape: [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
    "        if attn_mask is not None:\n",
    "             scores.masked_fill_(attn_mask.unsqueeze(1).unsqueeze(1) == 0, -1e9)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attn, v_s) # [batch_size, n_heads, seq_len, d_k]\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return self.layernorm(output + Q), attn # Residual connection + LayerNorm\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # GELU is standard for BERT (Smoother ReLU)\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.layernorm(x + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        x, attn_weights = self.attn(x, x, x, attn_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x, attn_weights\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, max_len, n_segments, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbedding(vocab_size, d_model, max_len, n_segments, dropout)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # MLM Head\n",
    "        # Projects back to vocab_size to predict the masked word\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Weight tying (optional but standard in BERT to reduce params)\n",
    "        # self.classifier.weight = self.embedding.tok_embed.weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, segment_ids=None):\n",
    "        # Handle missing segment_ids (default to all zeros)\n",
    "        if segment_ids is None:\n",
    "            segment_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output, attn = layer(output, attention_mask)\n",
    "        \n",
    "        # MLM Prediction\n",
    "        # We only predict the masked tokens, but for implementation simplicity\n",
    "        # we project the whole sequence and let the loss function handle the masking logic.\n",
    "        h_masked = self.norm(self.activ(self.linear(output)))\n",
    "        logits_lm = self.classifier(h_masked)\n",
    "        \n",
    "        return logits_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4dd877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on cuda\n",
      "\n",
      " Starting Training (MLM)...\n",
      "Epoch 1 | Step 100 | Loss: 7.2220\n",
      "Epoch 1 | Step 200 | Loss: 6.1597\n",
      "Epoch 1 | Step 300 | Loss: 6.9123\n",
      "Epoch 1 | Step 400 | Loss: 6.5726\n",
      "Epoch 1 | Step 500 | Loss: 6.1917\n",
      "Epoch 1 | Step 600 | Loss: 6.7937\n",
      "Epoch 1 | Step 700 | Loss: 6.3622\n",
      "Epoch 1 | Step 800 | Loss: 6.2571\n",
      "Epoch 1 | Step 900 | Loss: 6.2625\n",
      "Epoch 1 | Step 1000 | Loss: 6.0941\n",
      "Epoch 1 | Step 1100 | Loss: 6.4586\n",
      " Epoch 1 Completed | Average Loss: 6.6300\n",
      "Epoch 2 | Step 100 | Loss: 5.9137\n",
      "Epoch 2 | Step 200 | Loss: 5.8978\n",
      "Epoch 2 | Step 300 | Loss: 6.1429\n",
      "Epoch 2 | Step 400 | Loss: 5.4760\n",
      "Epoch 2 | Step 500 | Loss: 5.3608\n",
      "Epoch 2 | Step 600 | Loss: 5.4596\n",
      "Epoch 2 | Step 700 | Loss: 5.8484\n",
      "Epoch 2 | Step 800 | Loss: 4.5628\n",
      "Epoch 2 | Step 900 | Loss: 5.7266\n",
      "Epoch 2 | Step 1000 | Loss: 6.2032\n",
      "Epoch 2 | Step 1100 | Loss: 5.8064\n",
      " Epoch 2 Completed | Average Loss: 5.7019\n",
      "Epoch 3 | Step 100 | Loss: 5.3395\n",
      "Epoch 3 | Step 200 | Loss: 5.6606\n",
      "Epoch 3 | Step 300 | Loss: 5.4777\n",
      "Epoch 3 | Step 400 | Loss: 5.4430\n",
      "Epoch 3 | Step 500 | Loss: 5.7093\n",
      "Epoch 3 | Step 600 | Loss: 5.1148\n",
      "Epoch 3 | Step 700 | Loss: 4.8288\n",
      "Epoch 3 | Step 800 | Loss: 4.9826\n",
      "Epoch 3 | Step 900 | Loss: 5.4567\n",
      "Epoch 3 | Step 1000 | Loss: 4.9701\n",
      "Epoch 3 | Step 1100 | Loss: 5.5043\n",
      " Epoch 3 Completed | Average Loss: 5.2944\n",
      "Epoch 4 | Step 100 | Loss: 4.6251\n",
      "Epoch 4 | Step 200 | Loss: 4.8319\n",
      "Epoch 4 | Step 300 | Loss: 4.9450\n",
      "Epoch 4 | Step 400 | Loss: 5.0285\n",
      "Epoch 4 | Step 500 | Loss: 5.2348\n",
      "Epoch 4 | Step 600 | Loss: 4.9246\n",
      "Epoch 4 | Step 700 | Loss: 5.4234\n",
      "Epoch 4 | Step 800 | Loss: 5.4657\n",
      "Epoch 4 | Step 900 | Loss: 5.4666\n",
      "Epoch 4 | Step 1000 | Loss: 4.9199\n",
      "Epoch 4 | Step 1100 | Loss: 5.3061\n",
      " Epoch 4 Completed | Average Loss: 5.0375\n",
      "Epoch 5 | Step 100 | Loss: 4.6215\n",
      "Epoch 5 | Step 200 | Loss: 5.1763\n",
      "Epoch 5 | Step 300 | Loss: 4.5465\n",
      "Epoch 5 | Step 400 | Loss: 4.6098\n",
      "Epoch 5 | Step 500 | Loss: 4.9034\n",
      "Epoch 5 | Step 600 | Loss: 4.8280\n",
      "Epoch 5 | Step 700 | Loss: 4.5496\n",
      "Epoch 5 | Step 800 | Loss: 4.9824\n",
      "Epoch 5 | Step 900 | Loss: 4.9459\n",
      "Epoch 5 | Step 1000 | Loss: 4.6117\n",
      "Epoch 5 | Step 1100 | Loss: 5.0861\n",
      " Epoch 5 Completed | Average Loss: 4.8439\n",
      "Epoch 6 | Step 100 | Loss: 4.0292\n",
      "Epoch 6 | Step 200 | Loss: 5.1669\n",
      "Epoch 6 | Step 300 | Loss: 4.9865\n",
      "Epoch 6 | Step 400 | Loss: 4.5172\n",
      "Epoch 6 | Step 500 | Loss: 4.6873\n",
      "Epoch 6 | Step 600 | Loss: 4.8297\n",
      "Epoch 6 | Step 700 | Loss: 4.7456\n",
      "Epoch 6 | Step 800 | Loss: 4.4439\n",
      "Epoch 6 | Step 900 | Loss: 4.8337\n",
      "Epoch 6 | Step 1000 | Loss: 4.7285\n",
      "Epoch 6 | Step 1100 | Loss: 5.1786\n",
      " Epoch 6 Completed | Average Loss: 4.6707\n",
      "Epoch 7 | Step 100 | Loss: 4.6939\n",
      "Epoch 7 | Step 200 | Loss: 4.9516\n",
      "Epoch 7 | Step 300 | Loss: 4.7000\n",
      "Epoch 7 | Step 400 | Loss: 4.8153\n",
      "Epoch 7 | Step 500 | Loss: 4.2298\n",
      "Epoch 7 | Step 600 | Loss: 4.7615\n",
      "Epoch 7 | Step 700 | Loss: 4.4088\n",
      "Epoch 7 | Step 800 | Loss: 4.2569\n",
      "Epoch 7 | Step 900 | Loss: 4.2158\n",
      "Epoch 7 | Step 1000 | Loss: 4.5097\n",
      "Epoch 7 | Step 1100 | Loss: 4.5163\n",
      " Epoch 7 Completed | Average Loss: 4.5128\n",
      "Epoch 8 | Step 100 | Loss: 4.1670\n",
      "Epoch 8 | Step 200 | Loss: 4.9450\n",
      "Epoch 8 | Step 300 | Loss: 4.1412\n",
      "Epoch 8 | Step 400 | Loss: 4.7581\n",
      "Epoch 8 | Step 500 | Loss: 4.1186\n",
      "Epoch 8 | Step 600 | Loss: 4.4484\n",
      "Epoch 8 | Step 700 | Loss: 4.3653\n",
      "Epoch 8 | Step 800 | Loss: 4.5526\n",
      "Epoch 8 | Step 900 | Loss: 4.2882\n",
      "Epoch 8 | Step 1000 | Loss: 4.6584\n",
      "Epoch 8 | Step 1100 | Loss: 4.1121\n",
      " Epoch 8 Completed | Average Loss: 4.3809\n",
      "Epoch 9 | Step 100 | Loss: 4.5235\n",
      "Epoch 9 | Step 200 | Loss: 4.0262\n",
      "Epoch 9 | Step 300 | Loss: 3.9216\n",
      "Epoch 9 | Step 400 | Loss: 4.0262\n",
      "Epoch 9 | Step 500 | Loss: 4.0226\n",
      "Epoch 9 | Step 600 | Loss: 4.7288\n",
      "Epoch 9 | Step 700 | Loss: 3.8534\n",
      "Epoch 9 | Step 800 | Loss: 3.7640\n",
      "Epoch 9 | Step 900 | Loss: 4.4999\n",
      "Epoch 9 | Step 1000 | Loss: 4.0212\n",
      "Epoch 9 | Step 1100 | Loss: 4.5177\n",
      " Epoch 9 Completed | Average Loss: 4.2512\n",
      "Epoch 10 | Step 100 | Loss: 3.9673\n",
      "Epoch 10 | Step 200 | Loss: 4.0508\n",
      "Epoch 10 | Step 300 | Loss: 3.3348\n",
      "Epoch 10 | Step 400 | Loss: 3.8424\n",
      "Epoch 10 | Step 500 | Loss: 3.8323\n",
      "Epoch 10 | Step 600 | Loss: 4.2469\n",
      "Epoch 10 | Step 700 | Loss: 3.9972\n",
      "Epoch 10 | Step 800 | Loss: 4.4003\n",
      "Epoch 10 | Step 900 | Loss: 4.2084\n",
      "Epoch 10 | Step 1000 | Loss: 3.4482\n",
      "Epoch 10 | Step 1100 | Loss: 4.0632\n",
      " Epoch 10 Completed | Average Loss: 4.1515\n"
     ]
    }
   ],
   "source": [
    "# Training Setup & Execution\n",
    "\n",
    "\n",
    "# 1. Hyperparameters (Small BERT for faster training)\n",
    "VOCAB_SIZE = 30522 # Standard BERT tokenizer vocab size\n",
    "D_MODEL = 768      # Standard BERT hidden size\n",
    "N_LAYERS = 6       # Standard is 12, we use 6 for speed\n",
    "N_HEADS = 8        # Must divide D_MODEL (768/8 = 96)\n",
    "D_FF = 3072        # Standard BERT feed-forward size (4 * D_MODEL)\n",
    "MAX_LEN = 128\n",
    "N_SEGMENTS = 2\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 10         # Small dataset needs more epochs to converge\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# 2. Initialize Model\n",
    "model = BERT(VOCAB_SIZE, D_MODEL, N_LAYERS, N_HEADS, D_FF, MAX_LEN, N_SEGMENTS, DROPOUT)\n",
    "model.to(device)\n",
    "print(f\"Model initialized on {device}\")\n",
    "\n",
    "# 3. Optimizer & Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100) # Ignore non-masked tokens\n",
    "\n",
    "# 4. Training Loop\n",
    "print(\"\\n Starting Training (MLM)...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # Move batch to GPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Calculate Loss\n",
    "        # Flatten outputs: [batch_size * seq_len, vocab_size]\n",
    "        # Flatten labels:  [batch_size * seq_len]\n",
    "        loss = criterion(outputs.view(-1, VOCAB_SIZE), labels.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(f\"Epoch {epoch+1} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\" Epoch {epoch+1} Completed | Average Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61999c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model weights saved to bert_mlm_scratch.pth\n"
     ]
    }
   ],
   "source": [
    "# 5. Save the Model \n",
    "save_path = \"bert_mlm_scratch.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"\\n Model weights saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6777d86d",
   "metadata": {},
   "source": [
    "## Task 2. Sentence Embedding with Sentence BERT\n",
    "\n",
    "We need to train a Siamese Network to classify the relationship between two sentences (Entailment, Neutral, Contradiction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a9e00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SNLI dataset...\n",
      "Filtering and creating subset...\n",
      "SNLI Train Size: 50000\n",
      "SNLI Val Size:   9842\n",
      "SNLI Test Size:  9824\n",
      "\n",
      " SNLI DataLoaders ready.\n",
      "Premise Shape: torch.Size([32, 128])\n",
      "Label Shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# SNLI Data Loading & Preprocessing\n",
    "\n",
    "# 1. Load SNLI Dataset\n",
    "print(\"Loading SNLI dataset...\")\n",
    "snli_dataset = load_dataset('snli')\n",
    "\n",
    "# 2. Filter & Subset\n",
    "# - Remove entries with label -1 (which means annotators disagreed)\n",
    "# - Select a subset (e.g., 50k samples) to ensure training finishes quickly\n",
    "print(\"Filtering and creating subset...\")\n",
    "snli_dataset = snli_dataset.filter(lambda x: x['label'] != -1)\n",
    "\n",
    "# We use 50,000 samples for training to match the scale of Task 1\n",
    "train_subset = snli_dataset['train'].select(range(50000)) \n",
    "val_subset = snli_dataset['validation']\n",
    "test_subset = snli_dataset['test']\n",
    "\n",
    "print(f\"SNLI Train Size: {len(train_subset)}\")\n",
    "print(f\"SNLI Val Size:   {len(val_subset)}\")\n",
    "print(f\"SNLI Test Size:  {len(test_subset)}\")\n",
    "\n",
    "# 3. Custom Dataset for Sentence Pairs\n",
    "class SNLIDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "        premise = row['premise']\n",
    "        hypothesis = row['hypothesis']\n",
    "        label = row['label']\n",
    "\n",
    "        # Tokenize Premise (Sentence A)\n",
    "        encoding_a = self.tokenizer(\n",
    "            premise,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize Hypothesis (Sentence B)\n",
    "        encoding_b = self.tokenizer(\n",
    "            hypothesis,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': encoding_a['input_ids'].squeeze(0),\n",
    "            'attention_mask_a': encoding_a['attention_mask'].squeeze(0),\n",
    "            'input_ids_b': encoding_b['input_ids'].squeeze(0),\n",
    "            'attention_mask_b': encoding_b['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "# We reuse the tokenizer and MAX_LEN from Task 1\n",
    "train_snli_ds = SNLIDataset(train_subset, tokenizer, MAX_LEN)\n",
    "val_snli_ds = SNLIDataset(val_subset, tokenizer, MAX_LEN)\n",
    "test_snli_ds = SNLIDataset(test_subset, tokenizer, MAX_LEN)\n",
    "\n",
    "train_snli_loader = DataLoader(train_snli_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_snli_loader = DataLoader(val_snli_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_snli_loader = DataLoader(test_snli_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check a batch\n",
    "sample_snli = next(iter(train_snli_loader))\n",
    "print(f\"\\n SNLI DataLoaders ready.\")\n",
    "print(f\"Premise Shape: {sample_snli['input_ids_a'].shape}\")\n",
    "print(f\"Label Shape: {sample_snli['label'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013aac72",
   "metadata": {},
   "source": [
    "## The Siamese Network Architecture\n",
    "\n",
    "The original BERT architecture is not well-suited for sentence comparison because it requires feeding both sentences into the model simultaneously, which is computationally expensive. \n",
    "\n",
    "Sentence-BERT solves this by using a \"Siamese\" architecture:\n",
    "\n",
    "We pass Sentence A (Premise) and Sentence B (Hypothesis) through the same BERT encoder independently to get their hidden states.\n",
    "\n",
    "We use Mean Pooling to average the token embeddings into a single vector for each sentence ($u$ and $v$).\n",
    "\n",
    "We concatenate $u$, $v$, and the absolute difference $|u - v|$.\n",
    "\n",
    "We pass this concatenated vector into a Softmax classifier to predict Entailment, Neutral, or Contradiction.\n",
    "\n",
    "Here is the mathematical representation of the Classification Objective Function we are implementing:\n",
    "\n",
    "$o=softmax(W^{T}\\cdot(u,v,|u-v|))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47368f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese Network Initialized with weights from Task 1.\n"
     ]
    }
   ],
   "source": [
    "# Siamese Network Architecture\n",
    "\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies mean pooling to the token embeddings, ignoring padding tokens.\n",
    "    \"\"\"\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        # Expand attention mask to match hidden state dimensions\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        \n",
    "        # Sum the embeddings, but only for non-padding tokens\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        \n",
    "        # Count the number of non-padding tokens (clamp to avoid division by zero)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        # Calculate the mean\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "class SiameseNLI(nn.Module):\n",
    "    def __init__(self, pretrained_bert, d_model, num_classes=3):\n",
    "        super().__init__()\n",
    "        # 1. Extract the base encoder from our custom BERT\n",
    "        self.embedding = pretrained_bert.embedding\n",
    "        self.layers = pretrained_bert.layers\n",
    "        \n",
    "        # 2. Pooling layer\n",
    "        self.pooler = MeanPooling()\n",
    "        \n",
    "        # 3. Classification Head (Softmax Loss)\n",
    "        # The input is concatenated (u, v, |u-v|), so the dimension is 3 * d_model\n",
    "        self.classifier = nn.Linear(3 * d_model, num_classes)\n",
    "\n",
    "    def get_sentence_embedding(self, input_ids, attention_mask):\n",
    "        # Pass through the embedding layer\n",
    "        segment_ids = torch.zeros_like(input_ids) # No segments needed for single sentences\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        \n",
    "        # Pass through the Transformer layers\n",
    "        for layer in self.layers:\n",
    "            output, _ = layer(output, attention_mask)\n",
    "            \n",
    "        # Apply Mean Pooling\n",
    "        return self.pooler(output, attention_mask)\n",
    "\n",
    "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
    "        # 1. Get embeddings for Sentence A (u) and Sentence B (v)\n",
    "        # Because we use the same get_sentence_embedding function, the weights are tied!\n",
    "        u = self.get_sentence_embedding(input_ids_a, attention_mask_a)\n",
    "        v = self.get_sentence_embedding(input_ids_b, attention_mask_b)\n",
    "        \n",
    "        # 2. Calculate absolute difference |u - v|\n",
    "        uv_abs = torch.abs(u - v)\n",
    "        \n",
    "        # 3. Concatenate (u, v, |u-v|)\n",
    "        features = torch.cat([u, v, uv_abs], dim=1)\n",
    "        \n",
    "        # 4. Softmax Classifier\n",
    "        # Note: We return raw logits because PyTorch's CrossEntropyLoss \n",
    "        # applies LogSoftmax internally.\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits, u, v\n",
    "\n",
    "# Initialize the Siamese Network\n",
    "# We pass in the 'model' we trained in Task 1 so it inherits the learned weights\n",
    "siamese_model = SiameseNLI(model, D_MODEL, num_classes=3)\n",
    "siamese_model.to(device)\n",
    "\n",
    "print(\"Siamese Network Initialized with weights from Task 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18416320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Siamese Network Training (Fine-Tuning on SNLI)...\n",
      "Epoch 1 | Step 200/1563 | Loss: 1.0259 | Acc: 0.4512\n",
      "Epoch 1 | Step 400/1563 | Loss: 0.9197 | Acc: 0.4737\n",
      "Epoch 1 | Step 600/1563 | Loss: 0.9179 | Acc: 0.4914\n",
      "Epoch 1 | Step 800/1563 | Loss: 0.9106 | Acc: 0.5066\n",
      "Epoch 1 | Step 1000/1563 | Loss: 0.7016 | Acc: 0.5159\n",
      "Epoch 1 | Step 1200/1563 | Loss: 0.8719 | Acc: 0.5242\n",
      "Epoch 1 | Step 1400/1563 | Loss: 0.9880 | Acc: 0.5297\n",
      "Epoch 1 Train Completed | Avg Loss: 0.9528 | Train Acc: 0.5349\n",
      "Epoch 2 | Step 200/1563 | Loss: 0.6787 | Acc: 0.6253\n",
      "Epoch 2 | Step 400/1563 | Loss: 0.7992 | Acc: 0.6227\n",
      "Epoch 2 | Step 600/1563 | Loss: 0.9668 | Acc: 0.6220\n",
      "Epoch 2 | Step 800/1563 | Loss: 0.7900 | Acc: 0.6226\n",
      "Epoch 2 | Step 1000/1563 | Loss: 0.7235 | Acc: 0.6252\n",
      "Epoch 2 | Step 1200/1563 | Loss: 0.8005 | Acc: 0.6258\n",
      "Epoch 2 | Step 1400/1563 | Loss: 0.8410 | Acc: 0.6271\n",
      "Epoch 2 Train Completed | Avg Loss: 0.8278 | Train Acc: 0.6273\n"
     ]
    }
   ],
   "source": [
    "# Training the Siamese Network\n",
    "\n",
    "# 1. Hyperparameters for Fine-Tuning\n",
    "EPOCHS_NLI = 2          \n",
    "LEARNING_RATE_NLI = 2e-5\n",
    "\n",
    "# 2. Optimizer and Loss Function\n",
    "# We use CrossEntropyLoss which combines LogSoftmax and NLLLoss\n",
    "optimizer_nli = optim.Adam(siamese_model.parameters(), lr=LEARNING_RATE_NLI)\n",
    "criterion_nli = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3. Training Loop\n",
    "print(f\"Starting Siamese Network Training (Fine-Tuning on SNLI)...\")\n",
    "\n",
    "for epoch in range(EPOCHS_NLI):\n",
    "    siamese_model.train()\n",
    "    total_train_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_snli_loader):\n",
    "        # Move tensors to GPU\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer_nli.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, _, _ = siamese_model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion_nli(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer_nli.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy for monitoring\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Print progress every 200 steps\n",
    "        if step % 200 == 0 and step > 0:\n",
    "            current_acc = correct_predictions / total_samples\n",
    "            print(f\"Epoch {epoch+1} | Step {step}/{len(train_snli_loader)} | Loss: {loss.item():.4f} | Acc: {current_acc:.4f}\")\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = total_train_loss / len(train_snli_loader)\n",
    "    train_acc = correct_predictions / total_samples\n",
    "    print(f\"Epoch {epoch+1} Train Completed | Avg Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a851b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Siamese model weights saved to sbert_snli_scratch.pth\n"
     ]
    }
   ],
   "source": [
    "# 4. Save the Fine-Tuned Model\n",
    "save_path_nli = \"sbert_snli_scratch.pth\"\n",
    "torch.save(siamese_model.state_dict(), save_path_nli)\n",
    "print(f\"\\nSiamese model weights saved to {save_path_nli}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bda7f8",
   "metadata": {},
   "source": [
    "## Task 3. Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eeeebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set...\n",
      "\n",
      "==================================================\n",
      "             CLASSIFICATION REPORT\n",
      "==================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   entailment       0.61      0.75      0.68      3368\n",
      "      neutral       0.62      0.63      0.63      3219\n",
      "contradiction       0.69      0.51      0.59      3237\n",
      "\n",
      "     accuracy                           0.63      9824\n",
      "    macro avg       0.64      0.63      0.63      9824\n",
      " weighted avg       0.64      0.63      0.63      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation and Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Define the label mapping based on SNLI dataset\n",
    "# 0: entailment, 1: neutral, 2: contradiction\n",
    "label_names = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "# 2. Evaluation Function\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    print(\"Evaluating on Test Set...\")\n",
    "    with torch.no_grad(): # Disable gradient calculation for faster inference\n",
    "        for batch in dataloader:\n",
    "            # Move to GPU\n",
    "            input_ids_a = batch['input_ids_a'].to(device)\n",
    "            attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "            input_ids_b = batch['input_ids_b'].to(device)\n",
    "            attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Get raw logits from the model\n",
    "            logits, _, _ = model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
    "            \n",
    "            # The predicted class is the one with the highest logit score\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Store predictions and true labels\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    return all_true_labels, all_predictions\n",
    "\n",
    "# 3. Run Evaluation on the Test Set\n",
    "y_true, y_pred = evaluate_model(siamese_model, test_snli_loader)\n",
    "\n",
    "# 4. Generate Classification Report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"             CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "report = classification_report(y_true, y_pred, target_names=label_names, digits=2)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e8dfe",
   "metadata": {},
   "source": [
    "## Limitations and Potential Improvements\n",
    "\n",
    "### Documentation & Datasets\n",
    "* **Pre-training (Task 1):** I used the `wikitext-2-v1` dataset from Hugging Face as a manageable subset of Wikipedia. \n",
    "* **Fine-Tuning (Task 2):** I used the `snli` dataset from Hugging Face.\n",
    "* **Hyperparameters:** The BERT model was configured as a \"Small BERT\" (6 layers, 8 attention heads, 768 hidden size) to allow training from scratch within reasonable compute limits. Pre-training used a learning rate of 1e-4 for 10 epochs, while fine-tuning used 2e-5 for 2 epochs.\n",
    "\n",
    "### Limitations & Challenges\n",
    "1. **Dataset Size:** Implementing and training BERT from scratch on a small subset (approx. 100k samples) is conceptually correct, but practically limits the model's vocabulary and contextual understanding. Real BERT models train on over 3.3 billion words.\n",
    "2. **Compute Constraints:** Due to local hardware limits, I had to significantly reduce the number of Transformer layers (from 12 to 6) and attention heads (from 12 to 8). This reduces the model's capacity to capture complex sentence relationships.\n",
    "3. **Training Time:** The pre-training phase was short. Language models typically require millions of optimization steps to converge on Masked Language Modeling (MLM). \n",
    "4. **No NSP Task:** I exclusively used MLM for pre-training. The original BERT also uses Next Sentence Prediction (NSP), which might have helped the Siamese network better understand sentence-pair relationships later on.\n",
    "\n",
    "### Potential Improvements\n",
    "1. **Scale Up:** If given access to a computing cluster, I would train on the full English Wikipedia and BookCorpus with the standard 12-layer architecture.\n",
    "2. **Add MNLI Data:** As suggested in the assignment, supplementing the SNLI dataset with the MNLI dataset would introduce more diverse linguistic genres (e.g., telephone conversations, fiction), improving the model's generalization capabilities.\n",
    "3. **Longer Fine-Tuning:** Implementing a learning rate scheduler with warm-up steps and training for 4-5 epochs on the SNLI dataset could squeeze out slightly better classification accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
