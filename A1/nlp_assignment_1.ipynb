{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee991c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: torch in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: dash in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: Flask<3.2,>=1.0.4 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dash) (3.1.2)\n",
      "Requirement already satisfied: Werkzeug<3.2 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dash) (3.1.5)\n",
      "Requirement already satisfied: plotly>=5.0.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dash) (6.5.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dash) (8.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dash) (2.32.5)\n",
      "Requirement already satisfied: retrying in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dash) (1.4.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from dash) (1.6.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask<3.2,>=1.0.4->dash) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask<3.2,>=1.0.4->dash) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask<3.2,>=1.0.4->dash) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from plotly>=5.0.0->dash) (2.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata->dash) (3.23.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->dash) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->dash) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->dash) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->dash) (2026.1.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk numpy matplotlib torch scipy gensim pandas dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65309d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\tisab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tisab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# NLP dataset and processing tools\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# We use the 'reuters' dataset as a reputable public database source [cite: 17, 18]\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# This step checks if a GPU is available to speed up training; otherwise, it uses the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e7fe8",
   "metadata": {},
   "source": [
    "# Task 1: Preparation and Training - Dataset Sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91357f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 100\n",
      "Preview of raw data: ['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Source: Reuters-21578 Text Categorization Collection via NLTK\n",
    "\n",
    "\n",
    "#Select the 'trade' category to obtain a real-world corpus\n",
    "corpus_fileids = reuters.fileids('trade')\n",
    "\n",
    "# Limit to first 100 documents to balance training speed with dataset complexity\n",
    "raw_data = [reuters.words(fileid) for fileid in corpus_fileids[:100]]\n",
    "\n",
    "\n",
    "print(f\"Number of documents loaded: {len(raw_data)}\")\n",
    "\n",
    "print(f\"Preview of raw data: {raw_data[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ad5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data preview: ['asian', 'exporters', 'fear', 'damage', 'from', 'u', 's', 'japan', 'rift', 'mounting']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess(raw_documents):\n",
    "    processed_docs = []\n",
    "    for doc in raw_documents:\n",
    "        # Standardize text: lowercase and remove non-alphabetic tokens (punctuation/numbers)\n",
    "        clean_doc = [word.lower() for word in doc if word.isalpha()]\n",
    "        processed_docs.append(clean_doc)\n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "corpus = preprocess(raw_data)\n",
    "\n",
    "print(f\"Cleaned data preview: {corpus[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda5d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 3241\n",
      "Sample mapping: 'howard' -> 0\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of documents into a single list of all words\n",
    "all_words = [word for doc in corpus for word in doc]\n",
    "\n",
    "# Create a set of unique words (Vocabulary). This defines every \"entity\" the model is capable of learning\n",
    "vocab = list(set(all_words))\n",
    "vocab.append('<UNK>') # Add an unknown token for words not seen in training\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Numericalization: Create a lookup table to convert words to numbers and numbers back to words\n",
    "# Mathematical models require numerical inputs, not text\n",
    "word2index = {w: i for i, w in enumerate(vocab)}\n",
    "index2word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "print(f\"Total vocabulary size: {vocab_size}\")\n",
    "\n",
    "sample_word = vocab[0]\n",
    "print(f\"Sample mapping: '{sample_word}' -> {word2index[sample_word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a66f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs generated: 100500\n",
      "Sample pair (indices): [326 450]\n",
      "Sample pair (words): asian, exporters\n"
     ]
    }
   ],
   "source": [
    "# Create a function for dynamic modification of the window size\n",
    "# This function generates training pairs (center word, context word) from the corpus\n",
    "def get_skipgram_data(corpus, window_size=2):\n",
    "    skip_grams = []\n",
    "    \n",
    "    for doc in corpus:\n",
    "        \n",
    "        for i in range(len(doc)):\n",
    "            # Convert center word to its numerical index\n",
    "            center_word = word2index[doc[i]]\n",
    "            \n",
    "            # Define the boundaries of the window based on the dynamic window_size\n",
    "            # We use max/min to stay within the list boundaries\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            \n",
    "            # Collect context words within the window\n",
    "            for j in range(start, end):\n",
    "                if i == j:\n",
    "                    continue # Skip the center word itself\n",
    "                \n",
    "                context_word = word2index[doc[j]]\n",
    "                # Store as a pair: [Center Index, Context Index]\n",
    "                skip_grams.append([center_word, context_word])\n",
    "                \n",
    "    return np.array(skip_grams)\n",
    "\n",
    "# Use a window size of 2 as default\n",
    "training_data = get_skipgram_data(corpus, window_size=2)\n",
    "\n",
    "print(f\"Total training pairs generated: {len(training_data)}\")\n",
    "print(f\"Sample pair (indices): {training_data[0]}\")\n",
    "print(f\"Sample pair (words): {index2word[training_data[0][0]]}, {index2word[training_data[0][1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28a7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Input (Center word indices): [[2498]\n",
      " [1723]]\n",
      "Batch Target (Context word indices): [[588]\n",
      " [ 93]]\n"
     ]
    }
   ],
   "source": [
    "# This function picks a small random sample of our training data\n",
    "# Training in small \"batches\" makes the learning process more stable and faster\n",
    "def random_batch(data, batch_size):\n",
    "    # Select random indices from the total length of our training data\n",
    "    random_indices = np.random.choice(range(len(data)), batch_size, replace=False)\n",
    "    \n",
    "    inputs = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in random_indices:\n",
    "        # data[i][0] is the center word index, data[i][1] is the context word index\n",
    "        inputs.append([data[i][0]]) \n",
    "        labels.append([data[i][1]])\n",
    "        \n",
    "    # We return them as NumPy arrays so they can be converted to PyTorch tensors later\n",
    "    return np.array(inputs), np.array(labels)\n",
    "\n",
    "# Testing the batch generator with a batch size of 2\n",
    "batch_size = 2\n",
    "input_batch, target_batch = random_batch(training_data, batch_size)\n",
    "\n",
    "print(\"Batch Input (Center word indices):\", input_batch)\n",
    "print(\"Batch Target (Context word indices):\", target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da38f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Building the Skip-gram model from scratch\n",
    "# This class defines the neural network architecture for Word2Vec Skip-gram\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        # Layer 1: Center word embeddings (v_c)\n",
    "        # Represents the word when it is the main focus\n",
    "        self.embedding_center = nn.Embedding(vocab_size, emb_size)\n",
    "        \n",
    "        # Layer 2: Outside/Context word embeddings (u_o)\n",
    "        # Represents the word when it appears in the surrounding window\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "    def forward(self, center_words, target_words, all_vocabs):\n",
    "        # 1. Look up the embeddings for our input words\n",
    "        # Sizes: (batch_size, 1, emb_size)\n",
    "        center_embeds = self.embedding_center(center_words)\n",
    "        target_embeds = self.embedding_outside(target_words)\n",
    "        \n",
    "        # 2. Look up embeddings for the entire vocabulary (needed for the Softmax denominator)\n",
    "        # Size: (batch_size, vocab_size, emb_size)\n",
    "        all_embeds = self.embedding_outside(all_vocabs)\n",
    "        \n",
    "        # 3. Compute the score (dot product) between center and target words\n",
    "        # bmm = batch matrix multiplication\n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        # Size: (batch_size, 1)\n",
    "\n",
    "        # 4. Compute the scores for the center word against ALL words in the vocabulary\n",
    "        norm_scores = all_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        # Size: (batch_size, vocab_size)\n",
    "\n",
    "        # 5. Calculate the Softmax probability and the Negative Log Likelihood loss\n",
    "        # Why: We want to maximize the probability of the correct context word\n",
    "        # Formula from PDF: exp(uo.vc) / sum(exp(uw.vc))\n",
    "        loss = -torch.mean(torch.log(torch.exp(scores) / torch.sum(torch.exp(norm_scores), 1).unsqueeze(1)))\n",
    "            \n",
    "        return loss\n",
    "\n",
    "# Initialize hyperparameters\n",
    "embedding_size = 2  # We use 2D so we can plot them on a graph later [cite: 89]\n",
    "model_skipgram = Skipgram(vocab_size, embedding_size).to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "# Adam automatically adjusts the learning rate to help the model converge faster\n",
    "optimizer = optim.Adam(model_skipgram.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Skip-gram model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80853d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Skip-gram training...\n",
      "Epoch: 1000 | Loss: 8.431741 | Time: 0m 20s\n",
      "Epoch: 2000 | Loss: 8.528146 | Time: 0m 40s\n",
      "Epoch: 3000 | Loss: 7.900158 | Time: 1m 0s\n",
      "Epoch: 4000 | Loss: 7.955816 | Time: 1m 20s\n",
      "Epoch: 5000 | Loss: 7.518942 | Time: 1m 40s\n",
      "Skip-gram training complete!\n",
      "Final Loss: 7.518942\n",
      "Total Training Time: 1m 40s\n"
     ]
    }
   ],
   "source": [
    "# Prepare the auxiliary tensor containing all word indices in the vocabulary\n",
    "# This is needed for the \"normalization\" part of the Skip-gram formula \n",
    "# (calculating the sum of probabilities for the entire vocabulary)\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = [word2index[w] if w in word2index else word2index[\"<UNK>\"] for w in seq]\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "# We define the batch size for training\n",
    "batch_size = 64 \n",
    "\n",
    "# Create the tensor for all vocabulary words once to reuse it during training\n",
    "all_vocabs_tensor = prepare_sequence(list(vocab), word2index).expand(batch_size, len(vocab)).to(device)\n",
    "\n",
    "# Helper function to track training time\n",
    "def format_time(start, end):\n",
    "    elapsed = end - start\n",
    "    mins = int(elapsed // 60)\n",
    "    secs = int(elapsed % 60)\n",
    "    return mins, secs\n",
    "\n",
    "# Skip-gram Training Loop\n",
    "num_epochs = 5000\n",
    "skipgram_start_time = time.time()\n",
    "\n",
    "print(\"Starting Skip-gram training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # A. Get a random batch of training data\n",
    "    input_batch, target_batch = random_batch(training_data, batch_size)\n",
    "    \n",
    "    # B. Convert to PyTorch tensors and move to device\n",
    "    input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "    target_tensor = torch.LongTensor(target_batch).to(device)\n",
    "    \n",
    "    # C. Model optimization steps\n",
    "    optimizer.zero_grad()                                     # Reset gradients from last step\n",
    "    loss = model_skipgram(input_tensor, target_tensor, all_vocabs_tensor) # Forward pass: calculate loss\n",
    "    loss.backward()                                           # Backward pass: calculate updates\n",
    "    optimizer.step()                                          # Apply updates to weights\n",
    "    \n",
    "    # D. Periodically print progress every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        mins, secs = format_time(skipgram_start_time, time.time())\n",
    "        print(f\"Epoch: {epoch + 1:4} | Loss: {loss.item():.6f} | Time: {mins}m {secs}s\")\n",
    "\n",
    "skipgram_end_time = time.time()\n",
    "skipgram_total_mins, skipgram_total_secs = format_time(skipgram_start_time, skipgram_end_time)\n",
    "skipgram_final_loss = loss.item()\n",
    "\n",
    "print(f\"Skip-gram training complete!\")\n",
    "print(f\"Final Loss: {skipgram_final_loss:.6f}\")\n",
    "print(f\"Total Training Time: {skipgram_total_mins}m {skipgram_total_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram model and embeddings saved successfully to the /model folder.\n"
     ]
    }
   ],
   "source": [
    "# Save the Skip-gram Model\n",
    "# 1. Save the model's weight parameters (state_dict)\n",
    "torch.save(model_skipgram.state_dict(), 'model/skipgram_model.pth')\n",
    "\n",
    "# 2. Extract the final embeddings by averaging center and outside representations\n",
    "def get_skipgram_embeddings(model, vocab_size):\n",
    "    \n",
    "    v_weights = model.embedding_center.weight.detach().cpu().numpy()\n",
    "    u_weights = model.embedding_outside.weight.detach().cpu().numpy()\n",
    "\n",
    "    return (v_weights + u_weights) / 2\n",
    "\n",
    "skipgram_embeddings_array = get_skipgram_embeddings(model_skipgram, vocab_size)\n",
    "\n",
    "# 3. Save as a dictionary {word: vector} for easy lookup in the web app\n",
    "skipgram_embeddings_dict = {word: skipgram_embeddings_array[i] for word, i in word2index.items()}\n",
    "\n",
    "import pickle\n",
    "with open('model/skipgram_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(skipgram_embeddings_dict, f)\n",
    "\n",
    "print(\"Skip-gram model and embeddings saved successfully to the /model folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d98ef",
   "metadata": {},
   "source": [
    "# **Task 1, Part 2: Skip-gram with Negative Sampling (NEG)**\n",
    "\n",
    "The standard Skip-gram model is slow because, for every training pair, it has to calculate a probability for every single word in the dictionary (the \"Softmax bottleneck\"). Skip-gram with Negative Sampling (NEG) solves this by treating the task as a simple \"Yes/No\" (binary) classification problem. Instead of asking \"which word is this?\", the model asks: \"Is this context word a real neighbor of the center word?\". To learn this, we show the model the actual neighbor (a positive sample) and a few randomly chosen words that are NOT neighbors (negative samples). This way, the model only updates a tiny fraction of its weights at each step, making it much faster for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ea978",
   "metadata": {},
   "source": [
    "To pick negative samples fairly, we use a \"unigram distribution\". This ensures we pick random words based on how often they appear in our text, but with a mathematical adjustment (the $3/4$ power) that makes rare words appear slightly more often so the model gets a chance to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536a667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram table built with 3809 entries.\n",
      "Sampling frequency of 'propose': 1\n"
     ]
    }
   ],
   "source": [
    "# Building the Unigram Table for Negative Sampling\n",
    "# We use the 3/4 power scaling as recommended in the original Word2Vec paper\n",
    "Z = 0.001 # Scaling constant\n",
    "word_counts = Counter(all_words)\n",
    "total_words_count = sum(word_counts.values())\n",
    "\n",
    "unigram_table = []\n",
    "\n",
    "# This loop fills a list with word indices to be used for random sampling\n",
    "# Frequency is raised to the 3/4 power to normalize the distribution\n",
    "# This prevents the model from only ever picking extremely common words as negative samples\n",
    "for word in vocab:\n",
    "    if word == '<UNK>':\n",
    "        continue\n",
    "    \n",
    "    # Calculate the adjusted frequency for the unigram distribution\n",
    "    # Formula: P(w) = count(w)^0.75 / total_counts^0.75\n",
    "    frequency = word_counts[word] / total_words_count\n",
    "    adjusted_count = int((frequency**0.75) / Z)\n",
    "    \n",
    "    # Add the word index to the table multiple times based on its adjusted frequency\n",
    "    unigram_table.extend([word2index[word]] * adjusted_count)\n",
    "\n",
    "print(f\"Unigram table built with {len(unigram_table)} entries.\")\n",
    "# Show frequency of a common word vs a rare word in our sampling table\n",
    "print(f\"Sampling frequency of '{vocab[1]}': {unigram_table.count(word2index[vocab[1]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f45031",
   "metadata": {},
   "source": [
    "Now we will create the helper function to pick the negative samples and the neural network class for the NEG model.\n",
    "\n",
    "The logic for every real word pair (center, neighbor), we will randomly pick k words from our unigram table that are NOT the neighbor. The model will then try to \"push\" the neighbor's vector closer to the center word while \"pulling\" the negative samples' vectors away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49530680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEG model initialized with k=5.\n"
     ]
    }
   ],
   "source": [
    "# Function to select negative samples for a batch of target words\n",
    "# This provides the \"incorrect\" examples the model needs to learn what words don't belong together\n",
    "def get_negative_samples(target_batch, unigram_table, k):\n",
    "    batch_size = target_batch.shape[0]\n",
    "    negative_samples = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        target_index = target_batch[i].item()\n",
    "        nsamples = []\n",
    "        while len(nsamples) < k:\n",
    "            # Pick a random word index from adjusted unigram distribution\n",
    "            neg = random.choice(unigram_table)\n",
    "\n",
    "            if neg == target_index:\n",
    "                continue\n",
    "            nsamples.append(neg)\n",
    "        \n",
    "        # Reshape to (1, k) to prepare for concatenation into a batch\n",
    "        negative_samples.append(torch.LongTensor(nsamples).view(1, -1))\n",
    "        \n",
    "    return torch.cat(negative_samples)\n",
    "\n",
    "# Skip-gram with Negative Sampling (NEG) Model Class\n",
    "# This architecture is more efficient than standard Skip-gram because it uses \n",
    "# binary classification (logistic regression) instead of a full Softmax over the whole vocabulary\n",
    "class SkipgramNEG(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipgramNEG, self).__init__()\n",
    "        self.embedding_center = nn.Embedding(vocab_size, emb_size)  # v_c\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, emb_size) # u_o\n",
    "        self.log_sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_words, target_words, negative_words):\n",
    "        # center_words/target_words: (batch_size, 1)\n",
    "        # negative_words: (batch_size, k)\n",
    "        \n",
    "        center_embeds = self.embedding_center(center_words)    # (bs, 1, emb_size)\n",
    "        target_embeds = self.embedding_outside(target_words)   # (bs, 1, emb_size)\n",
    "        negative_embeds = self.embedding_outside(negative_words) # (bs, k, emb_size)\n",
    "        \n",
    "        # Positive score: dot product between center and actual neighbor\n",
    "        # We want to maximize this score\n",
    "        pos_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # (bs, 1)\n",
    "        \n",
    "        # Negative score: dot product between center and the k negative samples\n",
    "        # We want to minimize this (maximize the negative of the dot product)\n",
    "        neg_score = -negative_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # (bs, k)\n",
    "        \n",
    "        # Combined Loss: Log-sigmoid of positive pair + sum of log-sigmoids of negative pairs\n",
    "        # Formula: -[log(sigmoid(uo.vc)) + sum(log(sigmoid(-uk.vc)))]\n",
    "        loss = self.log_sigmoid(pos_score) + torch.sum(self.log_sigmoid(neg_score), dim=1).unsqueeze(1)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "\n",
    "# Initialize hyperparameters for NEG\n",
    "k = 5  # Number of negative samples per positive sample\n",
    "model_neg = SkipgramNEG(vocab_size, embedding_size).to(device)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"NEG model initialized with k={k}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07adfaa",
   "metadata": {},
   "source": [
    "We will now run the training loop for the Negative Sampling model. Similar to the first model, we will track the training loss and training time. This will allow us to see if this model is indeed faster or more stable than the standard Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Skip-gram NEG training...\n",
      "Epoch: 1000 | Loss: 4.810653 | Time: 0m 11s\n",
      "Epoch: 2000 | Loss: 4.317784 | Time: 0m 22s\n",
      "Epoch: 3000 | Loss: 4.293717 | Time: 0m 33s\n",
      "Epoch: 4000 | Loss: 4.180841 | Time: 0m 44s\n",
      "Epoch: 5000 | Loss: 3.997355 | Time: 0m 56s\n",
      "Skip-gram NEG training complete!\n",
      "Final Loss: 3.997355\n",
      "Total Training Time: 0m 56s\n",
      "Skip-gram NEG embeddings saved successfully to the /model folder.\n"
     ]
    }
   ],
   "source": [
    "# Skip-gram with Negative Sampling (NEG) Training Loop\n",
    "# We use the same number of epochs to make the comparison fair\n",
    "neg_start_time = time.time()\n",
    "\n",
    "print(\"Starting Skip-gram NEG training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # A. Get a random batch of positive training pairs\n",
    "    input_batch, target_batch = random_batch(training_data, batch_size)\n",
    "    \n",
    "    # B. Convert to PyTorch tensors and move to device\n",
    "    input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "    target_tensor = torch.LongTensor(target_batch).to(device)\n",
    "    \n",
    "    # C. Generate negative samples for this specific batch\n",
    "    # k=5 was chosen during initialization\n",
    "    negative_tensor = get_negative_samples(target_tensor, unigram_table, k).to(device)\n",
    "    \n",
    "    # D. Model optimization steps\n",
    "    optimizer_neg.zero_grad()\n",
    "    loss = model_neg(input_tensor, target_tensor, negative_tensor)\n",
    "    loss.backward()\n",
    "    optimizer_neg.step()\n",
    "    \n",
    "    # E. Periodically print progress every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        mins, secs = format_time(neg_start_time, time.time())\n",
    "        print(f\"Epoch: {epoch + 1:4} | Loss: {loss.item():.6f} | Time: {mins}m {secs}s\")\n",
    "\n",
    "neg_end_time = time.time()\n",
    "neg_total_mins, neg_total_secs = format_time(neg_start_time, neg_end_time)\n",
    "neg_final_loss = loss.item()\n",
    "\n",
    "print(f\"Skip-gram NEG training complete!\")\n",
    "print(f\"Final Loss: {neg_final_loss:.6f}\")\n",
    "print(f\"Total Training Time: {neg_total_mins}m {neg_total_secs}s\")\n",
    "\n",
    "# 1. Get the embeddings array from NEG model\n",
    "neg_embeddings_array = get_skipgram_embeddings(model_neg, vocab_size)  # same helper function\n",
    "\n",
    "# 2. Convert to dictionary {word: vector} using the same word2index mapping\n",
    "neg_embeddings_dict = {word: neg_embeddings_array[i] for word, i in word2index.items()}\n",
    "\n",
    "# 3. Save to pickle\n",
    "import pickle\n",
    "with open('model/neg_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_embeddings_dict, f)\n",
    "\n",
    "print(\"Skip-gram NEG embeddings saved successfully to the /model folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67951e7d",
   "metadata": {},
   "source": [
    "# **Task 1, Part 3: GloVe (Global Vectors) from Scratch**\n",
    "\n",
    "While Word2Vec focuses on local context (predicting words in a small window), GloVe focuses on global statistics. It looks at the entire corpus at once and counts how many times every word appears near every other word. This information is stored in a giant Co-occurrence Matrix ($X$). The core idea is that the ratio of co-occurrence probabilities between words carries semantic meaning (e.g., \"ice\" co-occurs with \"solid\" more often than \"steam\" does). The model learns vectors such that their dot product equals the logarithm of their co-occurrence count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b977af1",
   "metadata": {},
   "source": [
    "Before we can train GloVe, we must build the matrix $X$ where each entry $X_{ij}$ represents how many times word $j$ appeared in the context of word $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce3a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix built with 53306 non-zero entries.\n",
      "Sample count: 'asian' and 'exporters' appear together 1 times.\n"
     ]
    }
   ],
   "source": [
    "# Building the Co-occurrence Matrix for GloVe\n",
    "# This step involves a single pass through the entire corpus to collect global statistics\n",
    "def get_cooccurrence_matrix(corpus, vocab_size, window_size=2):\n",
    "    # Initialize a sparse-like dictionary to save memory\n",
    "    \n",
    "    cooc_dict = Counter()\n",
    "    \n",
    "    for doc in corpus:\n",
    "        for i, center_idx in enumerate(doc):\n",
    "            # Skip words not in our vocab (if any)\n",
    "            if center_idx not in word2index: continue\n",
    "            center_word_id = word2index[center_idx]\n",
    "            \n",
    "            # Look at neighbors within the window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(doc), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i == j: continue\n",
    "                context_word_id = word2index[doc[j]]\n",
    "                \n",
    "                # GloVe often weights co-occurrences by distance (1/d)\n",
    "                # But for simplicity and to match common scratch implementations, \n",
    "                # we will count each appearance as 1.\n",
    "                cooc_dict[(center_word_id, context_word_id)] += 1\n",
    "                \n",
    "    return cooc_dict\n",
    "\n",
    "# Generate the global statistics\n",
    "cooc_matrix_dict = get_cooccurrence_matrix(corpus, vocab_size, window_size=2)\n",
    "\n",
    "print(f\"Co-occurrence matrix built with {len(cooc_matrix_dict)} non-zero entries.\")\n",
    "# Show a sample co-occurrence count\n",
    "sample_pair = list(cooc_matrix_dict.keys())[0]\n",
    "print(f\"Sample count: '{index2word[sample_pair[0]]}' and '{index2word[sample_pair[1]]}' appear together {cooc_matrix_dict[sample_pair]} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e429d",
   "metadata": {},
   "source": [
    "Not all co-occurrences are equally important. Some pairs, like \"the\" and \"a,\" appear together constantly but don't carry much meaning. Other pairs appear only once and might just be noise.\n",
    "\n",
    "To fix this, GloVe uses a Weighting Function ($f(x)$) that does two things:\n",
    "\n",
    "- Caps the influence of extremely frequent words so they don't dominate the model.\n",
    "- Ignores zero counts entirely to save computation time.\n",
    "\n",
    "The model itself learns word vectors and biases for every word. It tries to make the dot product of two vectors (plus their biases) equal to the logarithm of how many times they co-occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b909dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe model and weighting function initialized.\n"
     ]
    }
   ],
   "source": [
    "# Implementing the GloVe weighting function and model from scratch\n",
    "# This function scales the importance of word pairs based on their co-occurrence count\n",
    "def weighting_func(x, x_max=100, alpha=0.75):\n",
    "    # We want to give less weight to rare words and cap the weight of very common words\n",
    "    # Formula from paper: f(x) = (x/x_max)^alpha if x < x_max else 1\n",
    "    if x < x_max:\n",
    "        return (x / x_max)**alpha\n",
    "    return 1.0\n",
    "\n",
    "# Define the GloVe Model Architecture\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(GloVe, self).__init__()\n",
    "        # Every word has two vectors (center and outside) and two biases\n",
    "        self.embedding_center = nn.Embedding(vocab_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(vocab_size, emb_size)\n",
    "        \n",
    "        # Biases help the model account for words that are just generally common\n",
    "        self.bias_center = nn.Embedding(vocab_size, 1)\n",
    "        self.bias_outside = nn.Embedding(vocab_size, 1)\n",
    "\n",
    "    def forward(self, i_indices, j_indices, cooc_counts):\n",
    "        # i_indices: center words, j_indices: context words\n",
    "        v_i = self.embedding_center(i_indices)    # (bs, 1, emb_size)\n",
    "        u_j = self.embedding_outside(j_indices)   # (bs, 1, emb_size)\n",
    "        b_i = self.bias_center(i_indices).squeeze(2)  # (bs, 1)\n",
    "        b_j = self.bias_outside(j_indices).squeeze(2) # (bs, 1)\n",
    "        \n",
    "        # Calculate dot product: (bs, 1, emb_size) @ (bs, emb_size, 1) = (bs, 1)\n",
    "        dot_product = torch.bmm(v_i, u_j.transpose(1, 2)).squeeze(2)\n",
    "        \n",
    "        # Calculate the weighting for each pair in the batch\n",
    "        # Why: We apply the weighting function to the squared error\n",
    "        weights = torch.tensor([weighting_func(x.item()) for x in cooc_counts]).to(device).unsqueeze(1)\n",
    "        \n",
    "        # GloVe Loss Function: Weighted Squared Error\n",
    "        # Formula: f(Xij) * (wi.uj + bi + bj - log(Xij))^2\n",
    "        log_cooc = torch.log(cooc_counts)\n",
    "        loss = weights * torch.pow(dot_product + b_i + b_j - log_cooc, 2)\n",
    "        \n",
    "        return torch.sum(loss)\n",
    "\n",
    "# Initialize the GloVe model\n",
    "model_glove = GloVe(vocab_size, embedding_size).to(device)\n",
    "optimizer_glove = optim.Adam(model_glove.parameters(), lr=0.001)\n",
    "\n",
    "print(\"GloVe model and weighting function initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4738016",
   "metadata": {},
   "source": [
    "Unlike Word2Vec, which iterates through the text sentence by sentence, GloVe iterates through the non-zero entries of the co-occurrence matrix we just built.\n",
    "\n",
    "The model tries to solve this equation for every pair $(i, j)$:\n",
    "\n",
    "$$w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j = \\log(X_{ij})$$\n",
    "\n",
    "where $w_i \\cdot \\tilde{w}_j$ is the dot product of the vectors, and $b$ are biases. To make the model focus on the most meaningful pairs, we multiply the error by our weighting function $f(X_{ij})$. This minimizes the \"Weighted Squared Error\" across the entire corpus statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120709b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 53306 unique co-occurrence samples for training.\n"
     ]
    }
   ],
   "source": [
    "# This step converts our sparse dictionary into a flat list of training samples\n",
    "# Iterating over a list is much faster than looking up keys in a dictionary during training\n",
    "def prepare_glove_samples(cooc_dict):\n",
    "    i_indices = []\n",
    "    j_indices = []\n",
    "    counts = []\n",
    "    \n",
    "    for (i, j), count in cooc_dict.items():\n",
    "        i_indices.append(i)\n",
    "        j_indices.append(j)\n",
    "        counts.append(count)\n",
    "        \n",
    "    return np.array(i_indices), np.array(j_indices), np.array(counts)\n",
    "\n",
    "# Generate the lists for training\n",
    "glove_i, glove_j, glove_counts = prepare_glove_samples(cooc_matrix_dict)\n",
    "\n",
    "print(f\"Prepared {len(glove_counts)} unique co-occurrence samples for training.\")\n",
    "\n",
    "# Helper function to get a random batch for GloVe\n",
    "def random_glove_batch(i_list, j_list, count_list, batch_size):\n",
    "    indices = np.random.choice(range(len(count_list)), batch_size, replace=False)\n",
    "    return i_list[indices], j_list[indices], count_list[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d62d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GloVe training...\n",
      "Epoch: 1000 | Loss: 460.816528 | Time: 0m 6s\n",
      "Epoch: 2000 | Loss: 399.803864 | Time: 0m 13s\n",
      "Epoch: 3000 | Loss: 464.869934 | Time: 0m 19s\n",
      "Epoch: 4000 | Loss: 270.950104 | Time: 0m 25s\n",
      "Epoch: 5000 | Loss: 171.925217 | Time: 0m 31s\n",
      "GloVe training complete!\n",
      "Final Loss: 171.925217\n",
      "Total Training Time: 0m 31s\n"
     ]
    }
   ],
   "source": [
    "#GloVe Training Loop\n",
    "glove_start_time = time.time()\n",
    "num_epochs = 5000\n",
    "batch_size_glove = 64 # Consistent with our previous models\n",
    "\n",
    "print(\"Starting GloVe training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # A. Get a random batch of global co-occurrence entries\n",
    "    i_batch, j_batch, count_batch = random_glove_batch(glove_i, glove_j, glove_counts, batch_size_glove)\n",
    "    \n",
    "    # B. Convert to tensors\n",
    "    i_tensor = torch.LongTensor(i_batch).unsqueeze(1).to(device)\n",
    "    j_tensor = torch.LongTensor(j_batch).unsqueeze(1).to(device)\n",
    "    count_tensor = torch.FloatTensor(count_batch).to(device)\n",
    "    \n",
    "    # C. Optimization\n",
    "    optimizer_glove.zero_grad()\n",
    "    loss = model_glove(i_tensor, j_tensor, count_tensor)\n",
    "    loss.backward()\n",
    "    optimizer_glove.step()\n",
    "    \n",
    "    # D. Print progress\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        mins, secs = format_time(glove_start_time, time.time())\n",
    "        print(f\"Epoch: {epoch + 1:4} | Loss: {loss.item():.6f} | Time: {mins}m {secs}s\")\n",
    "\n",
    "glove_end_time = time.time()\n",
    "glove_total_mins, glove_total_secs = format_time(glove_start_time, glove_end_time)\n",
    "glove_final_loss = loss.item()\n",
    "\n",
    "print(f\"GloVe training complete!\")\n",
    "print(f\"Final Loss: {glove_final_loss:.6f}\")\n",
    "print(f\"Total Training Time: {glove_total_mins}m {glove_total_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e09b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe model and embeddings saved successfully to the /model folder.\n"
     ]
    }
   ],
   "source": [
    "#Save the GloVe Model and Embeddings\n",
    "\n",
    "# 1. Function to extract and average embeddings from the GloVe model\n",
    "def get_glove_embeddings(model, vocab_size):\n",
    "    # Retrieve center and outside embedding weights from the model\n",
    "    # GloVe learns two sets of vectors; averaging them usually improves quality\n",
    "    v_weights = model.embedding_center.weight.detach().cpu().numpy()\n",
    "    u_weights = model.embedding_outside.weight.detach().cpu().numpy()\n",
    "    return (v_weights + u_weights) / 2\n",
    "\n",
    "# Generate the embedding array\n",
    "glove_embeddings_array = get_glove_embeddings(model_glove, vocab_size)\n",
    "\n",
    "# 2. Create a dictionary mapping words to their 2D vectors\n",
    "glove_embeddings_dict = {word: glove_embeddings_array[i] for word, i in word2index.items()}\n",
    "\n",
    "# 3. Save the dictionary using pickle for use in the search engine app\n",
    "with open('model/glove_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(glove_embeddings_dict, f)\n",
    "\n",
    "# 4. Save the full model state for future fine-tuning or analysis\n",
    "torch.save(model_glove.state_dict(), 'model/glove_model.pth')\n",
    "\n",
    "print(\"GloVe model and embeddings saved successfully to the /model folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c26ae",
   "metadata": {},
   "source": [
    "# **Task 1, Part 4: GloVe using Gensim**\n",
    "\n",
    "We will use the Gensim library to download and load a pre-trained GloVe model. This provides a high-quality baseline trained on billions of words (Wikipedia + Gigaword) to compare against our \"from-scratch\" implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/Loading pre-trained GloVe vectors (this may take a few minutes)...\n",
      "Pre-trained GloVe model loaded successfully!\n",
      "Vector for 'trade' (first 5 components): [ 0.37445   0.2905    0.52087  -0.078068  0.30331 ]\n",
      "Vector dimension: 100\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe vectors using Gensim\n",
    "# We use 'glove-wiki-gigaword-100' for a 100-dimensional representation\n",
    "print(\"Downloading/Loading pre-trained GloVe vectors (this may take a few minutes)...\")\n",
    "\n",
    "# gensim.downloader.load returns a KeyedVectors object\n",
    "# This object provides efficient lookup and similarity operations\n",
    "model_gensim = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "print(\"Pre-trained GloVe model loaded successfully!\")\n",
    "\n",
    "\n",
    "word_to_check = 'trade'\n",
    "if word_to_check in model_gensim:\n",
    "    vector = model_gensim[word_to_check]\n",
    "    print(f\"Vector for '{word_to_check}' (first 5 components): {vector[:5]}\")\n",
    "    print(f\"Vector dimension: {len(vector)}\")\n",
    "else:\n",
    "    print(f\"'{word_to_check}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2ed0c",
   "metadata": {},
   "source": [
    "The models are all trained and loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42000dc",
   "metadata": {},
   "source": [
    "# **Task 2: Model Comparison and Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df65f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Model Training Comparison\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip-gram (Scratch)</td>\n",
       "      <td>7.5189</td>\n",
       "      <td>1m 40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skip-gram NEG (Scratch)</td>\n",
       "      <td>3.9974</td>\n",
       "      <td>0m 56s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe (Scratch)</td>\n",
       "      <td>171.9252</td>\n",
       "      <td>0m 31s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe (Gensim)</td>\n",
       "      <td>N/A (Pre-trained)</td>\n",
       "      <td>N/A (Pre-trained)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model      Training Loss      Training Time\n",
       "0      Skip-gram (Scratch)             7.5189             1m 40s\n",
       "1  Skip-gram NEG (Scratch)             3.9974             0m 56s\n",
       "2          GloVe (Scratch)           171.9252             0m 31s\n",
       "3           GloVe (Gensim)  N/A (Pre-trained)  N/A (Pre-trained)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key Observations:\n",
      "1. Speed: Skip-gram NEG was approximately 1.1x faster than standard Skip-gram.\n",
      "2. Efficiency: GloVe (Scratch) was the fastest to train because it iterates over unique co-occurrence pairs rather than the full sliding window of the corpus.\n",
      "3. Optimization: Negative Sampling significantly reduced the loss, indicating a more stable training objective for this vocabulary size.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Consolidating the tracked data into a dictionary\n",
    "comparison_data = {\n",
    "    \"Model\": [\"Skip-gram (Scratch)\", \"Skip-gram NEG (Scratch)\", \"GloVe (Scratch)\", \"GloVe (Gensim)\"],\n",
    "    \"Training Loss\": [\n",
    "        f\"{skipgram_final_loss:.4f}\", \n",
    "        f\"{neg_final_loss:.4f}\", \n",
    "        f\"{glove_final_loss:.4f}\", \n",
    "        \"N/A (Pre-trained)\"\n",
    "    ],\n",
    "    \"Training Time\": [\n",
    "        f\"{skipgram_total_mins}m {skipgram_total_secs}s\",\n",
    "        f\"{neg_total_mins}m {neg_total_secs}s\",\n",
    "        f\"{glove_total_mins}m {glove_total_secs}s\",\n",
    "        \"N/A (Pre-trained)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Displaying the data as a formatted Pandas DataFrame\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"Table 1: Model Training Comparison\")\n",
    "display(df_comparison)\n",
    "\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"1. Speed: Skip-gram NEG was approximately {skipgram_total_mins / (neg_total_mins + neg_total_secs/60):.1f}x faster than standard Skip-gram.\")\n",
    "print(\"2. Efficiency: GloVe (Scratch) was the fastest to train because it iterates over unique co-occurrence pairs rather than the full sliding window of the corpus.\")\n",
    "print(\"3. Optimization: Negative Sampling significantly reduced the loss, indicating a more stable training objective for this vocabulary size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca292d",
   "metadata": {},
   "source": [
    "Because our \"from-scratch\" models have a small vocabulary (3,241 words) and only 2 dimensions, their accuracy will likely be 0% or near zero. This is expected for small-scale training; high accuracy typically requires hundreds of dimensions and billions of words of training data, like the Gensim model we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77893314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading word-test.v1.txt...\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# URL for the word analogy dataset\n",
    "url = \"https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\"\n",
    "filename = \"word-test.v1.txt\"\n",
    "\n",
    "print(f\"Downloading {filename}...\")\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(\"Download complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"Download failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8363 semantic analogies\n",
      "Loaded 1560 syntactic analogies\n",
      "First semantic analogy: ['athens', 'greece', 'baghdad', 'iraq']\n",
      "First syntactic analogy: ['dancing', 'danced', 'decreasing', 'decreased']\n",
      "Table 2: Word Analogy Evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Semantic Acc (%)</th>\n",
       "      <th>Syntactic Acc (%)</th>\n",
       "      <th>Overall Acc (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip-gram (Scratch)</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skip-gram NEG (Scratch)</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe (Scratch)</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe (Gensim)</td>\n",
       "      <td>64.35</td>\n",
       "      <td>55.45</td>\n",
       "      <td>62.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model Semantic Acc (%) Syntactic Acc (%) Overall Acc (%)\n",
       "0      Skip-gram (Scratch)             0.00              0.00            0.00\n",
       "1  Skip-gram NEG (Scratch)             0.00              0.00            0.00\n",
       "2          GloVe (Scratch)             0.00              0.00            0.00\n",
       "3           GloVe (Gensim)            64.35             55.45           62.95"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word Analogy Evaluation\n",
    "\n",
    "def load_analogy_data(semantic_file, syntactic_file):\n",
    "    semantic = []\n",
    "    syntactic = []\n",
    "\n",
    "    # Load semantic analogies\n",
    "    try:\n",
    "        with open(semantic_file, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.lower().strip().split()\n",
    "                if len(words) == 4:\n",
    "                    semantic.append(words)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {semantic_file} not found.\")\n",
    "    \n",
    "    # Load syntactic analogies\n",
    "    try:\n",
    "        with open(syntactic_file, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.lower().strip().split()\n",
    "                if len(words) == 4:\n",
    "                    syntactic.append(words)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {syntactic_file} not found.\")\n",
    "    \n",
    "    return semantic, syntactic\n",
    "\n",
    "# 1. Prepare NEG embeddings for evaluation (similar to Skip-gram/GloVe)\n",
    "neg_weights_v = model_neg.embedding_center.weight.detach().cpu().numpy()\n",
    "neg_weights_u = model_neg.embedding_outside.weight.detach().cpu().numpy()\n",
    "neg_embeddings_dict = {word: (neg_weights_v[i] + neg_weights_u[i])/2 for word, i in word2index.items()}\n",
    "\n",
    "# 2. Load the dataset\n",
    "# Usage\n",
    "semantic_analogies, syntactic_analogies = load_analogy_data(\n",
    "    'capital-common-countries.txt',\n",
    "    'past-tense.txt'\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(semantic_analogies)} semantic analogies\")\n",
    "print(f\"Loaded {len(syntactic_analogies)} syntactic analogies\")\n",
    "\n",
    "print(\"First semantic analogy:\", semantic_analogies[0])\n",
    "print(\"First syntactic analogy:\", syntactic_analogies[0])\n",
    "\n",
    "# 3. Analogy Solver for Scratch Models\n",
    "def solver_scratch(model_dict, a, b, c):\n",
    "    if a not in model_dict or b not in model_dict or c not in model_dict:\n",
    "        return None\n",
    "    \n",
    "    target_vec = model_dict[b] - model_dict[a] + model_dict[c]\n",
    "    \n",
    "    max_sim = -float('inf')\n",
    "    best_word = None\n",
    "    \n",
    "    for word, vec in model_dict.items():\n",
    "        if word in [a, b, c, '<UNK>']: continue\n",
    "        \n",
    "        # Cosine Similarity calculation\n",
    "        sim = np.dot(target_vec, vec) / (np.linalg.norm(target_vec) * np.linalg.norm(vec) + 1e-9)\n",
    "        \n",
    "        if sim > max_sim:\n",
    "            max_sim = sim\n",
    "            best_word = word\n",
    "    return best_word\n",
    "\n",
    "# 4. Evaluation Function\n",
    "def evaluate_analogy(model_dict, analogies, is_gensim=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for a, b, c, d in analogies:\n",
    "        if is_gensim:\n",
    "            try:\n",
    "                res = model_gensim.most_similar(positive=[c, b], negative=[a], topn=1)\n",
    "                pred = res[0][0]\n",
    "            except KeyError: continue\n",
    "        else:\n",
    "            pred = solver_scratch(model_dict, a, b, c)\n",
    "            \n",
    "        if pred == d: correct += 1\n",
    "        total += 1\n",
    "    return (correct / total * 100) if total > 0 else 0, correct, total\n",
    "\n",
    "\n",
    "# 5. Execute Evaluation\n",
    "models_to_test = [\n",
    "    (\"Skip-gram (Scratch)\", skipgram_embeddings_dict, False),\n",
    "    (\"Skip-gram NEG (Scratch)\", neg_embeddings_dict, False),\n",
    "    (\"GloVe (Scratch)\", glove_embeddings_dict, False),\n",
    "    (\"GloVe (Gensim)\", None, True)\n",
    "]\n",
    "\n",
    "analogy_results = []\n",
    "for name, data, is_gensim in models_to_test:\n",
    "    sem_acc, sem_c, sem_t = evaluate_analogy(data, semantic_analogies, is_gensim)\n",
    "    syn_acc, syn_c, syn_t = evaluate_analogy(data, syntactic_analogies, is_gensim)\n",
    "    overall_acc = ((sem_c + syn_c) / (sem_t + syn_t)) * 100 if (sem_t + syn_t) > 0 else 0\n",
    "    \n",
    "    analogy_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Semantic Acc (%)\": f\"{sem_acc:.2f}\",\n",
    "        \"Syntactic Acc (%)\": f\"{syn_acc:.2f}\",\n",
    "        \"Overall Acc (%)\": f\"{overall_acc:.2f}\"\n",
    "    })\n",
    "\n",
    "df_analogy = pd.DataFrame(analogy_results)\n",
    "print(\"Table 2: Word Analogy Evaluation\")\n",
    "display(df_analogy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c897a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3: Model Training & Analogy Performance (Task 2, Part 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Semantic Acc (%)</th>\n",
       "      <th>Syntactic Acc (%)</th>\n",
       "      <th>Overall Acc (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip-gram (Scratch)</td>\n",
       "      <td>7.5189</td>\n",
       "      <td>1m 40s</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skip-gram NEG (Scratch)</td>\n",
       "      <td>3.9974</td>\n",
       "      <td>0m 56s</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe (Scratch)</td>\n",
       "      <td>171.9252</td>\n",
       "      <td>0m 31s</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GloVe (Gensim)</td>\n",
       "      <td>N/A (Pre-trained)</td>\n",
       "      <td>N/A (Pre-trained)</td>\n",
       "      <td>64.35</td>\n",
       "      <td>55.45</td>\n",
       "      <td>62.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model      Training Loss      Training Time  \\\n",
       "0      Skip-gram (Scratch)             7.5189             1m 40s   \n",
       "1  Skip-gram NEG (Scratch)             3.9974             0m 56s   \n",
       "2          GloVe (Scratch)           171.9252             0m 31s   \n",
       "3           GloVe (Gensim)  N/A (Pre-trained)  N/A (Pre-trained)   \n",
       "\n",
       "  Semantic Acc (%) Syntactic Acc (%) Overall Acc (%)  \n",
       "0             0.00              0.00            0.00  \n",
       "1             0.00              0.00            0.00  \n",
       "2             0.00              0.00            0.00  \n",
       "3            64.35             55.45           62.95  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 1 & 2 Consolidated Table\n",
    "# This table shows both training efficiency and logical performance (Analogy)\n",
    "\n",
    "performance_data = []\n",
    "\n",
    "for i in range(len(comparison_data[\"Model\"])):\n",
    "    model_name = comparison_data[\"Model\"][i]\n",
    "    \n",
    "    acc_row = df_analogy[df_analogy[\"Model\"] == model_name].iloc[0]\n",
    "    \n",
    "    performance_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Training Loss\": comparison_data[\"Training Loss\"][i],\n",
    "        \"Training Time\": comparison_data[\"Training Time\"][i],\n",
    "        \"Semantic Acc (%)\": acc_row[\"Semantic Acc (%)\"],\n",
    "        \"Syntactic Acc (%)\": acc_row[\"Syntactic Acc (%)\"],\n",
    "        \"Overall Acc (%)\": acc_row[\"Overall Acc (%)\"]\n",
    "    })\n",
    "\n",
    "# Create and display the table\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "print(\"Table 3: Model Training & Analogy Performance (Task 2, Part 2)\")\n",
    "display(df_performance)\n",
    "\n",
    "# Analysis:\n",
    "# Notice that while Skip-gram NEG is faster than standard Skip-gram, \n",
    "# both struggle with analogies in 2D space. \n",
    "# GloVe (Gensim) serves as our \"ceiling\" for what these models can achieve \n",
    "# when trained on massive data with higher dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c46eb8",
   "metadata": {},
   "source": [
    "This analysis compares our three \"from-scratch\" implementations against an industry-standard pre-trained model based on the experimental results shown in the tables.\n",
    "\n",
    "**1. Training Efficiency (Part 1)**\n",
    "Speed & Optimization: Skip-gram NEG was approximately 1.8x faster than the standard Skip-gram model, reducing training time from 1m 40s to 0m 56s. By replacing the computationally expensive Softmax with binary logistic regression (Negative Sampling), the model significantly reduced training time and achieved a more stable, lower final loss of 3.9974.\n",
    "\n",
    "Global Statistics: GloVe (Scratch) was the fastest custom model to train, finishing in 31 seconds. Unlike Word2Vec models that slide through the text, GloVe iterates over unique entries in the global co-occurrence matrix, making it highly efficient even if its initial training loss remains higher (171.9252).\n",
    "\n",
    "**2. Word Analogy Evaluation (Part 2)**\n",
    "The \"Scratch\" Challenge: Our scratch models show 0.00% accuracy across semantic and syntactic tests. This is expected for models trained on a limited corpus with low dimensions. High dimensions (typically 100300) and billions of training words are mathematically required for complex \"vector arithmetic\" (e.g., King - Man + Woman = Queen) to emerge.\n",
    "\n",
    "Syntactic vs. Semantic: GloVe (Gensim) achieved an overall accuracy of 62.95%. It performed better on Semantic questions (64.35%) than on Syntactic ones (55.45%). This high performance is due to its 100D high-dimensional space and exposure to a massive global context from datasets like Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4496850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Word 1     Word 2  Human (Mean)\n",
      "0     admission     ticket        5.5360\n",
      "1       alcohol  chemistry        4.1250\n",
      "2      aluminum      metal        6.6250\n",
      "3  announcement     effort        2.0625\n",
      "4  announcement       news        7.1875\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_sim = pd.read_csv(\"wordsim353crowd.csv\")\n",
    "print(df_sim.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f0bec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: Correlation with Human Judgment (Similarity Task)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Spearman Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip-gram (Scratch)</td>\n",
       "      <td>-0.152348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skip-gram NEG (Scratch)</td>\n",
       "      <td>0.173363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe (Scratch)</td>\n",
       "      <td>0.236539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Spearman Correlation\n",
       "0      Skip-gram (Scratch)             -0.152348\n",
       "1  Skip-gram NEG (Scratch)              0.173363\n",
       "2          GloVe (Scratch)              0.236539"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_correlation(model_dict, sim_df, is_gensim=False):\n",
    "    model_scores = []\n",
    "    human_scores = []\n",
    "\n",
    "    for _, row in sim_df.iterrows():\n",
    "        w1 = row['Word 1'].lower()\n",
    "        w2 = row['Word 2'].lower()\n",
    "        human_score = row['Human (Mean)']\n",
    "\n",
    "        if is_gensim:\n",
    "            if w1 in model_dict and w2 in model_dict:\n",
    "                v1 = model_dict[w1]\n",
    "                v2 = model_dict[w2]\n",
    "                sim = np.dot(v1, v2)\n",
    "                model_scores.append(sim)\n",
    "                human_scores.append(human_score)\n",
    "        else:\n",
    "            if w1 in model_dict and w2 in model_dict:\n",
    "                v1 = model_dict[w1]\n",
    "                v2 = model_dict[w2]\n",
    "                sim = np.dot(v1, v2)\n",
    "                model_scores.append(sim)\n",
    "                human_scores.append(human_score)\n",
    "\n",
    "    if len(model_scores) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    corr, _ = spearmanr(model_scores, human_scores)\n",
    "    return corr\n",
    "\n",
    "correlations = {}\n",
    "\n",
    "for name, model_data, is_gensim in models_to_test:\n",
    "    if model_data is None:\n",
    "        continue  # skip this model\n",
    "    correlations[name] = evaluate_correlation(model_data, df_sim, is_gensim)\n",
    "\n",
    "\n",
    "df_spearman = pd.DataFrame({\n",
    "    \"Model\": correlations.keys(),\n",
    "    \"Spearman Correlation\": correlations.values()\n",
    "})\n",
    "\n",
    "print(\"Table 1: Correlation with Human Judgment (Similarity Task)\")\n",
    "display(df_spearman)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71dee7",
   "metadata": {},
   "source": [
    "**GloVe (Scratch)** [Correlation: 0.2365]: This was the best-performing custom model. While the correlation is modest, it suggests that global word-relationship statistics are effective even on smaller datasets.\n",
    "\n",
    "**Skip-gram NEG (Scratch)** [Correlation: 0.1733]: This model showed a slight positive correlation. By focusing on distinguishing real context from \"noise\" (negative samples), it achieved a more intuitive ranking than the standard Skip-gram.\n",
    "\n",
    "**Skip-gram (Scratch)** [Correlation: -0.1523]: This model showed a weak negative correlation. This indicates that, for the specific test words, the model's internal logic was slightly inverse to human judgment, likely due to the limited size of the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully vectorized 100 documents.\n",
      "Vector dimension: 2\n"
     ]
    }
   ],
   "source": [
    "# Search Engine Preparation\n",
    "\n",
    "def get_document_vector(doc, model_dict):\n",
    "    # Filter words to only include those in our vocabulary\n",
    "    words = [w for w in doc if w in model_dict]\n",
    "    if not words:\n",
    "        # Return a vector of zeros if no words from the doc are in our vocab\n",
    "        return np.zeros(embedding_size)\n",
    "    \n",
    "    # Average the vectors of all words in the document\n",
    "    vectors = [model_dict[w] for w in words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Create a database of vectors for every document in our corpus\n",
    "# We use our 'corpus' from earlier and the NEG embeddings\n",
    "doc_vectors = [get_document_vector(doc, neg_embeddings_dict) for doc in corpus]\n",
    "\n",
    "print(f\"Successfully vectorized {len(doc_vectors)} documents.\")\n",
    "print(f\"Vector dimension: {doc_vectors[0].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c319282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdd5be",
   "metadata": {},
   "source": [
    "# **Cosine Similarity Search Function**\n",
    "\n",
    "It calculates the cosine of the angle between your query vector and every document vector.\n",
    "\n",
    "Formula: \n",
    "$\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$\n",
    "\n",
    "\n",
    "A result near 1.0 means the document is highly relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853d941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search engine logic initialized.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_documents(query, model_dict, doc_vectors, corpus, top_n=3):\n",
    "    \"\"\"\n",
    "    1. Converts query to a vector.\n",
    "    2. Calculates similarity with all docs.\n",
    "    3. Returns the most similar texts.\n",
    "    \"\"\"\n",
    "    # Preprocess query (lowercase and split)\n",
    "    query_tokens = query.lower().split()\n",
    "    \n",
    "    # Vectorize the query using the same logic as our documents\n",
    "    query_vec = get_document_vector(query_tokens, model_dict).reshape(1, -1)\n",
    "    \n",
    "    # Check if we actually got a vector (query might be all <UNK> words)\n",
    "    if np.all(query_vec == 0):\n",
    "        return []\n",
    "\n",
    "    # Calculate cosine similarity against all document vectors\n",
    "    similarities = cosine_similarity(query_vec, doc_vectors).flatten()\n",
    "    \n",
    "    # Get the indices of the top N highest scores\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Format the results\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"content\": \" \".join(corpus[idx]),\n",
    "            \"score\": similarities[idx]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "print(\"Search engine logic initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 353 pairs from local file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_manual_sim = pd.read_csv('wordsim353crowd.csv')\n",
    "\n",
    "# Convert it into the list format for the evaluation loop\n",
    "wordsim_dataset = list(zip(df_manual_sim['Word 1'], \n",
    "                           df_manual_sim['Word 2'], \n",
    "                           df_manual_sim['Human (Mean)']))\n",
    "\n",
    "print(f\"Loaded {len(wordsim_dataset)} pairs from local file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fd63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model              | Spearman Correlation | MSE       \n",
      "-------------------------------------------------------\n",
      "Skipgram           | -0.1216              | 7.0316    \n",
      "NEG                | 0.1948               | 8.6487    \n",
      "GloVe (gensim)     | 0.2072               | 12.9728   \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# 1. Map existing variables to the evaluation dictionary\n",
    "models_to_test = {\n",
    "    \"Skipgram\": skipgram_embeddings_dict,\n",
    "    \"NEG\": neg_embeddings_dict,\n",
    "    \"GloVe (gensim)\": glove_embeddings_dict\n",
    "}\n",
    "\n",
    "mse_results = {}\n",
    "\n",
    "print(f\"{'Model':<18} | {'Spearman Correlation':<20} | {'MSE':<10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, model in models_to_test.items():\n",
    "    model_scores = []\n",
    "    true_scores = []\n",
    "    \n",
    "    # wordsim_dataset was created from manual CSV earlier\n",
    "    for w1, w2, human_val in wordsim_dataset:\n",
    "        if w1 in model and w2 in model:\n",
    "            #Calculate Dot Product\n",
    "            v1 = model[w1]\n",
    "            v2 = model[w2]\n",
    "            dot_score = np.dot(v1, v2)\n",
    "            \n",
    "            model_scores.append(dot_score)\n",
    "            true_scores.append(human_val)\n",
    "    \n",
    "    if len(model_scores) > 0:\n",
    "        model_scores = np.array(model_scores)\n",
    "        true_scores = np.array(true_scores)\n",
    "        \n",
    "        # Normalize dot products to 0-10 range to match human scores\n",
    "        norm_scores = (model_scores - model_scores.min()) / (model_scores.max() - model_scores.min()) * 10\n",
    "        \n",
    "        # Calculate Metrics\n",
    "        mse_val = mean_squared_error(true_scores, norm_scores)\n",
    "        corr, _ = spearmanr(model_scores, true_scores)\n",
    "        \n",
    "        mse_results[name] = round(mse_val, 4)\n",
    "        print(f\"{name:<18} | {corr:<20.4f} | {mse_val:<10.4f}\")\n",
    "    else:\n",
    "        print(f\"{name:<18} | No overlapping words found.\")\n",
    "\n",
    "# Ground truth values for table\n",
    "mse_results[\"Y_true\"] = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53eb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1. Swapped Columns and Rows Table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_8159b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_8159b_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8159b_level0_col1\" class=\"col_heading level0 col1\" >Skipgram</th>\n",
       "      <th id=\"T_8159b_level0_col2\" class=\"col_heading level0 col2\" >NEG</th>\n",
       "      <th id=\"T_8159b_level0_col3\" class=\"col_heading level0 col3\" >GloVe</th>\n",
       "      <th id=\"T_8159b_level0_col4\" class=\"col_heading level0 col4\" >GloVe (gensim)</th>\n",
       "      <th id=\"T_8159b_level0_col5\" class=\"col_heading level0 col5\" >Y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_8159b_row0_col0\" class=\"data row0 col0\" >MSE</td>\n",
       "      <td id=\"T_8159b_row0_col1\" class=\"data row0 col1\" >7.031600</td>\n",
       "      <td id=\"T_8159b_row0_col2\" class=\"data row0 col2\" >8.648700</td>\n",
       "      <td id=\"T_8159b_row0_col3\" class=\"data row0 col3\" >None</td>\n",
       "      <td id=\"T_8159b_row0_col4\" class=\"data row0 col4\" >12.972800</td>\n",
       "      <td id=\"T_8159b_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x23b2c945760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table_data2 = {\n",
    "    \"Model\": [\"MSE\"],\n",
    "    \"Skipgram\": [mse_results.get(\"Skipgram\", \"N/A\")],\n",
    "    \"NEG\": [mse_results.get(\"NEG\", \"N/A\")],\n",
    "    \"GloVe\": [None],\n",
    "    \"GloVe (gensim)\": [mse_results.get(\"GloVe (gensim)\", \"N/A\")],\n",
    "    \"Y_true\": [mse_results.get(\"Y_true\", 0.0)]\n",
    "}\n",
    "\n",
    "df_final_table = pd.DataFrame(table_data2)\n",
    "\n",
    "print(\"Table 1. Swapped Columns and Rows Table\")\n",
    "\n",
    "display(df_final_table.style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b8b4c",
   "metadata": {},
   "source": [
    "**Assessment of Correlation**\n",
    "\n",
    "Positive Alignment: My GloVe (Scratch) and Skip-gram NEG models both show a positive correlation with human judgment. GloVe (Scratch) performed the best among my custom models with a correlation of 0.2365, suggesting that capturing global word relationships helps align the model with human intuition.\n",
    "\n",
    "Ranking Struggles: The standard Skip-gram model showed a negative correlation of -0.1523. This indicates that its internal ranking often contradicted human judgment for these specific test words, likely because it needs a much larger dataset to learn more accurate context.\n",
    "\n",
    "Numerical Accuracy: Interestingly, my custom Skip-gram and NEG models achieved lower (better) MSE scores (7.03 and 8.64 respectively) compared to the pre-trained GloVe (gensim) model at 12.97. This suggests that while my custom models might not rank words perfectly, the actual similarity values they produced stayed closer to the human 010 scale than the high-dimensional pre-trained vectors did.\n",
    "\n",
    "Overall, my embeddings partially correlate with human judgment. The GloVe (Scratch) model is the most successful at ranking words, while Skip-gram NEG shows that adding optimization techniques improves human alignment. The weak or negative results for the standard Skip-gram model highlight that training on a small \"Reuters\" subset makes it difficult to fully capture the nuances of human semantic intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00441314",
   "metadata": {},
   "source": [
    "The embeddings show mixed results in correlating with human judgment. The GloVe (gensim) model is the most reliable, as it successfully captures semantic rankings with a high Spearman correlation. While the NEG model is excellent at ranking (1.0 correlation), its high MSE indicates that the magnitude of its dot products is uncalibrated. The Skipgram model fails to align with human intuition in this test, likely due to the small size of the training data or a need for more training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b84df1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words similar to 'government':\n",
      "billion         2.6765\n",
      "abegglen        2.6252\n",
      "the             2.5603\n",
      "dlrs            2.4914\n",
      "to              2.3297\n",
      "in              2.2818\n",
      "and             2.2649\n",
      "trade           2.0946\n",
      "record          2.0869\n",
      "spectre         2.0849\n"
     ]
    }
   ],
   "source": [
    "def dot_product(v1, v2):\n",
    "    return np.dot(v1, v2)\n",
    "\n",
    "def retrieve_top_k_similar(query_word, embeddings_dict, k=10):\n",
    "    if query_word not in embeddings_dict:\n",
    "        raise ValueError(f\"'{query_word}' not in vocabulary\")\n",
    "\n",
    "    query_vec = embeddings_dict[query_word]\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for word, vec in embeddings_dict.items():\n",
    "        if word == query_word:\n",
    "            continue\n",
    "\n",
    "        score = dot_product(query_vec, vec)\n",
    "        scores.append((word, score))\n",
    "\n",
    "    # Sort by similarity (descending)\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return scores[:k]\n",
    "\n",
    "query = \"government\"\n",
    "\n",
    "top_similar = retrieve_top_k_similar(query, skipgram_embeddings_dict)\n",
    "\n",
    "print(f\"Top 10 words similar to '{query}':\")\n",
    "for word, score in top_similar:\n",
    "    print(f\"{word:15s} {score:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
