{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4918728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.10.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.10.0%2Bcpu-cp312-cp312-win_amd64.whl (113.7 MB)\n",
      "   ---------------------------------------- 0.0/113.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.4/113.7 MB 16.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 5.8/113.7 MB 16.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 9.2/113.7 MB 16.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 13.1/113.7 MB 17.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 16.8/113.7 MB 17.3 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 20.7/113.7 MB 17.9 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 23.3/113.7 MB 17.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 26.2/113.7 MB 16.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 29.9/113.7 MB 16.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 33.3/113.7 MB 16.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 37.5/113.7 MB 16.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 41.4/113.7 MB 17.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 44.8/113.7 MB 17.0 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 48.5/113.7 MB 17.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 52.2/113.7 MB 17.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 56.9/113.7 MB 17.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 60.6/113.7 MB 17.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 65.3/113.7 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 68.7/113.7 MB 17.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 72.6/113.7 MB 17.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 76.5/113.7 MB 17.8 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 80.5/113.7 MB 17.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 83.4/113.7 MB 17.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 85.2/113.7 MB 17.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 87.0/113.7 MB 16.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 88.6/113.7 MB 16.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 89.4/113.7 MB 16.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 90.2/113.7 MB 15.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 91.0/113.7 MB 15.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 91.8/113.7 MB 14.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 92.5/113.7 MB 14.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 93.8/113.7 MB 14.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 95.7/113.7 MB 13.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 97.8/113.7 MB 13.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 99.9/113.7 MB 13.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 102.8/113.7 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 105.9/113.7 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 109.3/113.7 MB 13.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  112.5/113.7 MB 13.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  113.5/113.7 MB 13.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 113.7/113.7 MB 13.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.10.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.24.1 requires torch==2.9.1, but you have torch 2.10.0+cpu which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc22fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (1.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tisab\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\tisab\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329af25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "# Check for GPU availability\n",
    "# If 'cuda' is present, we use the graphics card for faster training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# This ensures the model learns the same way every time we restart\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302d050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt...\n",
      "Download successful.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9aba9763db14ea497c4d48a20448738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Data structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 8561\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1070\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1071\n",
      "    })\n",
      "})\n",
      "\n",
      "Example text:\n",
      "{'text': ''}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# 1. Download the file manually from a public GitHub repo\n",
    "# Using Book 1 (Sorcerer's Stone).\n",
    "url = \"https://raw.githubusercontent.com/amephraim/nlp/master/texts/J.%20K.%20Rowling%20-%20Harry%20Potter%201%20-%20Sorcerer's%20Stone.txt\"\n",
    "file_path = \"harry_potter_book1.txt\"\n",
    "\n",
    "print(f\"Downloading from {url}...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Download successful.\")\n",
    "else:\n",
    "    print(\"Failed to download. Using dummy data for testing.\")\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(\"Harry Potter is a wizard. He goes to Hogwarts. He has friends named Ron and Hermione.\")\n",
    "\n",
    "# 2. Load the local file into the library\n",
    "dataset = load_dataset('text', data_files={'train': file_path})\n",
    "\n",
    "# 3. Split the data (Train 80% / Validation 10% / Test 10%)\n",
    "# First split: 80% Train, 20% Temporary\n",
    "train_testvalid = dataset['train'].train_test_split(test_size=0.2, seed=1234)\n",
    "\n",
    "# Second split: Split the 20% into half (10% Valid, 10% Test)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=1234)\n",
    "\n",
    "# Combine into final structure\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'validation': test_valid['train'],\n",
    "    'test': test_valid['test']\n",
    "})\n",
    "\n",
    "print(\"\\nSuccess! Data structure:\")\n",
    "print(dataset)\n",
    "print(\"\\nExample text:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc0bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Test: ['harry', 'potter', 'is', 'a', 'wizard', '!']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71608542d640492fbaf31fa29b81c1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a156d3ae23bf4e86adad8bf6f9e6bcb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1070 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56637a023ac4d8da564dd3aec13a95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Total words in vocabulary: 2154\n",
      "First 10 words: ['<unk>', '<eos>', 'bell', '-', 'hit', 'hard', 'in', 'the', 'face', 'by']\n",
      "Vocabulary saved to models/vocab_lm.pkl\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- 1. Create a Custom Tokenizer ---\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    # Find all words (alphanumeric) OR punctuation marks\n",
    "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "    return tokens\n",
    "\n",
    "print(f\"Tokenizer Test: {tokenizer('Harry Potter is a wizard!')}\")\n",
    "\n",
    "# --- 2. Tokenize the Dataset ---\n",
    "def tokenize_function(example):\n",
    "    return {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, remove_columns=['text'])\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# --- 3. Build the Vocabulary Manually ---\n",
    "class Vocabulary:\n",
    "    def __init__(self, vocab_dict, unk_idx):\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.unk_idx = unk_idx\n",
    "        # Create a reverse lookup (Number -> Word)\n",
    "        self.itos = {v: k for k, v in vocab_dict.items()}\n",
    "        \n",
    "    def __getitem__(self, token):\n",
    "        # Returns the ID for a word. If unknown, returns the <unk> ID.\n",
    "        return self.vocab_dict.get(token, self.unk_idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab_dict)\n",
    "    \n",
    "    def get_itos(self):\n",
    "        # Returns a list of all words\n",
    "        return [self.itos[i] for i in range(len(self))]\n",
    "\n",
    "# Count all words in the training set\n",
    "token_counts = collections.Counter()\n",
    "for tokens in tokenized_dataset['train']['tokens']:\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "# Create the dictionary starting with special tokens\n",
    "# <unk> = 0 (Unknown), <eos> = 1 (End of Sentence)\n",
    "vocab_dict = {'<unk>': 0, '<eos>': 1}\n",
    "index = 2\n",
    "\n",
    "# Add words that appear at least 3 times\n",
    "for token, count in token_counts.items():\n",
    "    if count >= 3:\n",
    "        vocab_dict[token] = index\n",
    "        index += 1\n",
    "\n",
    "# Initialize the Vocabulary object\n",
    "vocab = Vocabulary(vocab_dict, unk_idx=0)\n",
    "\n",
    "print(f\"Total words in vocabulary: {len(vocab)}\")\n",
    "print(f\"First 10 words: {vocab.get_itos()[:10]}\")\n",
    "\n",
    "# --- 4. Save the Vocabulary ---\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "    \n",
    "with open('models/vocab_lm.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Vocabulary saved to models/vocab_lm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([128, 695])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            # We add <eos> to the end so the model learns when a sentence finishes \n",
    "            tokens = example['tokens'] + ['<eos>']\n",
    "            # Convert words to their ID numbers using our vocab dictionary \n",
    "            tokens = [vocab[token] for token in tokens]\n",
    "            data.extend(tokens)\n",
    "            \n",
    "    # Convert the big list of numbers into a PyTorch \"Tensor\"\n",
    "    data = torch.LongTensor(data)\n",
    "    \n",
    "    # Calculate how many full batches we can make \n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    \n",
    "    # Trim off the extra numbers that don't fit into a full batch \n",
    "    data = data[:num_batches * batch_size]\n",
    "    \n",
    "    # Reshape the data so it's organized by batch \n",
    "    data = data.view(batch_size, num_batches)\n",
    "    return data\n",
    "\n",
    "# We will use a batch size of 128\n",
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65054674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 21,207,146 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('./app')\n",
    "from lstm import LSTMLanguageModel\n",
    "\n",
    "# 1. Define Settings (Hyperparameters)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024 # The 'size' of each word vector [cite: 10]\n",
    "hid_dim = 1024 # The 'memory' capacity of the LSTM [cite: 10]\n",
    "num_layers = 2 # How many LSTMs are stacked together [cite: 10]\n",
    "dropout_rate = 0.65 # Prevents the model from just memorizing the text [cite: 10]\n",
    "lr = 1e-3 # Learning rate: how big the 'steps' are during learning [cite: 10]\n",
    "\n",
    "# 2. Initialize the model on your CPU\n",
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "\n",
    "# 3. Define Optimizer and Loss Function\n",
    "# Adam is the tool that adjusts the weights to fix mistakes \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# CrossEntropyLoss measures how wrong the word predictions are \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Count how many total 'brain cells' the model has\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8c2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # For the progress bar\n",
    "\n",
    "# Helper to grab a sequence of words for the model to read\n",
    "def get_batch(data, seq_len, idx):\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1] # The next word we want to predict [cite: 12]\n",
    "    return src, target\n",
    "\n",
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train() # Set to learning mode\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size, device) # Start with fresh memory\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ', leave=False):\n",
    "        optimizer.zero_grad() # Reset the mistake counter\n",
    "        hidden = model.detach_hidden(hidden) # Clean memory for this step\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        \n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        # Reshape to compare predicted word vs actual word\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward() # Calculate how to fix the mistakes\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # Prevent math errors\n",
    "        optimizer.step() # Update the brain weights\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "        \n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval() # Set to testing mode (no learning)\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad(): # Don't update weights during testing\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8453d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTrain Perplexity: 647.617\n",
      "\tValid Perplexity: 205.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "\tTrain Perplexity: 224.915\n",
      "\tValid Perplexity: 148.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "\tTrain Perplexity: 156.073\n",
      "\tValid Perplexity: 112.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "\tTrain Perplexity: 121.784\n",
      "\tValid Perplexity: 92.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "\tTrain Perplexity: 101.807\n",
      "\tValid Perplexity: 81.458\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "n_epochs = 5\n",
    "seq_len  = 35 # Words to read in one breath\n",
    "clip     = 0.25\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# Learning rate scheduler: slows down learning if progress stops \n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        if not os.path.exists('models'): os.makedirs('models')\n",
    "        torch.save(model.state_dict(), 'models/best-val-lstm_lm.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6508792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Perplexity: 77.570\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# map_location=device ensures it loads correctly on our CPU\n",
    "model.load_state_dict(torch.load('models/best-val-lstm_lm.pt', map_location=device))\n",
    "\n",
    "# 2. Run the model on the Test Data\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "# 3. Print the final score\n",
    "print(f'Final Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4532e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: harry potter is one sobbed shelves wailed glanced tracks jacket wailed cheek men feathers halfway jacket planets countryside strangely sleeve glanced stumped planets planets astronomy whipped shout sweaty immediately countryside planets stumped apothecary\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_seq_len, temperature, model, tokenizer, vocab, device):\n",
    "    model.eval() # Set to evaluation mode\n",
    "    \n",
    "    # 1. Convert prompt into numbers\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    \n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    # 2. Start predicting words one by one\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            # 3. Use 'Temperature' to control creativity\n",
    "            # Lower temperature = safe/boring. Higher = creative/risky.\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            # If the model predicts <unk>, try again to get a real word\n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            # If the model predicts <eos>, the sentence is finished\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) # Feed the new word back in to get the next one\n",
    "\n",
    "    # 4. Convert the numbers back into words\n",
    "    itos = vocab.get_itos()\n",
    "    result_tokens = [itos[i] for i in indices]\n",
    "    return \" \".join(result_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43456bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Harry potter is \n",
      "==============================\n",
      "Temperature: 0.5\n",
      "harry potter is clothes complain smiles ollivanders excitement excitement planets tape jacket bezoar weather planets scream excitement excitement planets shelf shelf bushy nightmare jacket planets excitement bezoar shelves excitement countryside planets jacket planets\n",
      "------------------------------\n",
      "Temperature: 0.7\n",
      "harry potter is pay jacket setting bushy planets sleeve men wailed shelf fortune men sixteen countryside shelf fireplace flopped excitement whistle computer planets planets marks jacket bezoar easier quicker excitement greatest lip sleeve\n",
      "------------------------------\n",
      "Temperature: 0.75\n",
      "harry potter is top constrictor damp bushy whistle rare ignore flushed planets whistled stumped planets countryside whistled excitement trousers greatest scales cheek search shelves bezoar daily yell postcard planets men countryside rise curtains\n",
      "------------------------------\n",
      "Temperature: 0.8\n",
      "harry potter is mercy babble tucked stumped bezoar earlier choice bezoar fireplace jacket bezoar bott tongue planets witchcraft yellowish tape planets weather bushy shelf eight tape protect flopped quicker smiles bell nodding smiles\n",
      "------------------------------\n",
      "Temperature: 1.0\n",
      "harry potter is listen cheek sleeve doormat tabby wailed month tallest yellowish sleeve pick packages traffic friendly clambered fields traffic blame jacket blinked postcard clutched explained planets quicker postcard quicker owned bezoar animals\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup the parameters for the experiment\n",
    "prompt = 'Harry potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0 # Using a seed ensures we can compare the temperatures fairly\n",
    "\n",
    "# 2. Define the temperatures to test\n",
    "# Smaller = more predictable; Higher = more diverse/random\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\" + \"=\"*30)\n",
    "\n",
    "for temperature in temperatures:\n",
    "   \n",
    "    # We split it into a list of words\n",
    "    generation = generate_text(prompt, max_seq_len, temperature, model, tokenizer, vocab, device)\n",
    "    \n",
    "    # Printing the result for each temperature\n",
    "    print(f\"Temperature: {temperature}\")\n",
    "    print(generation)\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
